{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./401ksubs.csv')\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A:\n",
    "Property value, no. of properties owned, home address, car ownership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A:\n",
    "Racial discrimination/profiling. Are we implying certain races earn more, and some earn less?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A:\n",
    "1. obviously incsq: It is derived from inc ('income').\n",
    "\n",
    "2. e401k, e401k: Many people whether rich or poor can be eligible for and can participate in a 401k plan. These are not strong enough discriminating features.\n",
    "\n",
    "\"As long as your employer offers a 401(k) plan... then most employees are eligible to participate... require employers to let employees participate if they're at least 21 years old and have completed at least one year of service with that employer... many part-time employees end up being eligible even under the strictest plan guidelines,\"\n",
    "\n",
    "https://www.fool.com/retirement/how-to-open-a-401k-account-retirement.aspx\n",
    "\n",
    "3. pira: likewise for IRA accounts \n",
    "\n",
    "\"If you (or your spouse) earn taxable income and are under age 70 Â½, you can contribute. It's as easy as that.\"\n",
    "\n",
    "https://money.cnn.com/retirement/guide/IRA_traditional.moneymag/index3.htm?iid=EL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['e401k','p401k','pira','inc','incsq'])\n",
    "y = df['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9275 entries, 0 to 9274\n",
      "Data columns (total 6 columns):\n",
      "marr      9275 non-null int64\n",
      "male      9275 non-null int64\n",
      "age       9275 non-null int64\n",
      "fsize     9275 non-null int64\n",
      "nettfa    9275 non-null float64\n",
      "agesq     9275 non-null int64\n",
      "dtypes: float64(1), int64(5)\n",
      "memory usage: 434.8 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9275 entries, 0 to 9274\n",
      "Data columns (total 1 columns):\n",
      "inc    9275 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 72.5 KB\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(y).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.204420</td>\n",
       "      <td>41.080216</td>\n",
       "      <td>2.885067</td>\n",
       "      <td>19.071675</td>\n",
       "      <td>1793.652722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.483213</td>\n",
       "      <td>0.403299</td>\n",
       "      <td>10.299517</td>\n",
       "      <td>1.525835</td>\n",
       "      <td>63.963838</td>\n",
       "      <td>895.648841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-502.302000</td>\n",
       "      <td>625.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1089.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>18.449500</td>\n",
       "      <td>2304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1536.798000</td>\n",
       "      <td>4096.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              marr         male          age        fsize       nettfa  \\\n",
       "count  9275.000000  9275.000000  9275.000000  9275.000000  9275.000000   \n",
       "mean      0.628571     0.204420    41.080216     2.885067    19.071675   \n",
       "std       0.483213     0.403299    10.299517     1.525835    63.963838   \n",
       "min       0.000000     0.000000    25.000000     1.000000  -502.302000   \n",
       "25%       0.000000     0.000000    33.000000     2.000000    -0.500000   \n",
       "50%       1.000000     0.000000    40.000000     3.000000     2.000000   \n",
       "75%       1.000000     0.000000    48.000000     4.000000    18.449500   \n",
       "max       1.000000     1.000000    64.000000    13.000000  1536.798000   \n",
       "\n",
       "             agesq  \n",
       "count  9275.000000  \n",
       "mean   1793.652722  \n",
       "std     895.648841  \n",
       "min     625.000000  \n",
       "25%    1089.000000  \n",
       "50%    1600.000000  \n",
       "75%    2304.000000  \n",
       "max    4096.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figures look sensible. 'nettfa' has some negative figures but that's sensible as some people may have huge debts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A:\n",
    "income^2 and age^2. 'age' must have been a strong predictor of income (higher salary with seniority), and whether one is going to be a rich client for 401k plans. Likewise, 'income' itself is directly a strong predictor as to whether one is a rich client for 401k plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A:\n",
    "'inc' and 'age' both are wrongly labelled as 'inc^2' and 'age^2' respectively. Also, 'inc' should be in the thousands (just like nett total financial assets), hence should be labelled 'inc ($1000)'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regression models \n",
    "1. single or multiple linreg/lasso/ridge. Suitable to predict income, as they work well with continuous target variables like income.\n",
    "2. or decision trees. Also suitable, as Decision trees work well with either categorical (DecisionTreeClassifier, BaggingClassifier) or numerical (DecisionTreeRegressor, BaggingRegressor) target variables. However, for income (a continuous target variable) we need to segment it into bins via DecisionTreeRegressor, but in doing so, we may lose information.\n",
    "https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb\n",
    "3. or KNeighborsRegressor. Also suitable for continuous variables, like income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.2925748369332385\n",
      "Test Score : 0.27494825151995306\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marr</th>\n",
       "      <td>21.298405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>3.083980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>3.022602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nettfa</th>\n",
       "      <td>0.124052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agesq</th>\n",
       "      <td>-0.035157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fsize</th>\n",
       "      <td>-2.150561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef\n",
       "marr    21.298405\n",
       "age      3.083980\n",
       "male     3.022602\n",
       "nettfa   0.124052\n",
       "agesq   -0.035157\n",
       "fsize   -2.150561"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline linreg model\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "print('Train Score:',linreg.score(X_train, y_train))\n",
    "print('Test Score :',linreg.score(X_test, y_test))\n",
    "y_train_pred_linreg = linreg.predict(X_train)\n",
    "y_test_pred_linreg = linreg.predict(X_test)\n",
    "pd.DataFrame(linreg.coef_,X_train.columns, columns=['coef']).sort_values(by='coef',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " How about RidgeCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 0.1\n",
      "Train Score: 0.2925748358292586\n",
      "Test Score : 0.2749480419503103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marr</th>\n",
       "      <td>21.296346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>3.083894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>3.021861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nettfa</th>\n",
       "      <td>0.124053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agesq</th>\n",
       "      <td>-0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fsize</th>\n",
       "      <td>-2.150224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef\n",
       "marr    21.296346\n",
       "age      3.083894\n",
       "male     3.021861\n",
       "nettfa   0.124053\n",
       "agesq   -0.035156\n",
       "fsize   -2.150224"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linreg (ridgecv) model\n",
    "ridgecv_alphas = np.linspace(0.1,0.1,10)\n",
    "ridgecv = RidgeCV(alphas=ridgecv_alphas, cv=5)\n",
    "ridgecv.fit(X_train, y_train)\n",
    "print('Optimal alpha:',ridgecv.alpha_)\n",
    "print('Train Score:',ridgecv.score(X_train, y_train))\n",
    "print('Test Score :',ridgecv.score(X_test, y_test))\n",
    "y_train_pred_ridgecv = ridgecv.predict(X_train)\n",
    "y_test_pred_ridgecv = ridgecv.predict(X_test)\n",
    "pd.DataFrame(ridgecv.coef_,X_train.columns, columns=['coef']).sort_values(by='coef',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No better than baseline score. How about LassoCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 1.2589254117941673\n",
      "Train Score: 0.2635671104439756\n",
      "Test Score : 0.24447888033462417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marr</th>\n",
       "      <td>11.225632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.844117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nettfa</th>\n",
       "      <td>0.130447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fsize</th>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agesq</th>\n",
       "      <td>-0.020785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef\n",
       "marr    11.225632\n",
       "age      1.844117\n",
       "nettfa   0.130447\n",
       "male     0.000000\n",
       "fsize   -0.000000\n",
       "agesq   -0.020785"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linreg (lassocv) model\n",
    "lassocv_alphas = np.logspace(0.1,0.1,100)\n",
    "lassocv = LassoCV(alphas=lassocv_alphas, cv=5, random_state=42)\n",
    "lassocv.fit(X_train, y_train)\n",
    "print('Optimal alpha:',lassocv.alpha_)\n",
    "print('Train Score:',lassocv.score(X_train, y_train))\n",
    "print('Test Score :',lassocv.score(X_test, y_test))\n",
    "y_train_pred_lassocv = lassocv.predict(X_train)\n",
    "y_test_pred_lassocv = lassocv.predict(X_test)\n",
    "pd.DataFrame(lassocv.coef_,X_train.columns, columns=['coef']).sort_values(by='coef',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No better than baseline score.  How about knn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'algorithm': 'kd_tree', 'leaf_size': 50, 'n_neighbors': 15, 'weights': 'uniform'}\n",
      "Train Score: 0.3854519210087112\n",
      "Test Score : 0.2790084891137822\n"
     ]
    }
   ],
   "source": [
    "# knn\n",
    "knn = KNeighborsRegressor()\n",
    "params = {'n_neighbors':[10,15,30],\n",
    "          'weights':['uniform','distance'],\n",
    "          'algorithm':['ball_tree','kd_tree'],\n",
    "          'leaf_size':[30,50,80]}\n",
    "gs = GridSearchCV(knn,param_grid=params,cv=5,n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best params:',gs.best_params_)\n",
    "print('Train Score:',gs.score(X_train, y_train))\n",
    "print('Test Score :',gs.score(X_test, y_test))\n",
    "knn_best = gs.best_estimator_\n",
    "y_train_pred_knn_best = knn_best.predict(X_train)\n",
    "y_test_pred_knn_best = knn_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No better than baseline score. How about a single decision tree regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 5, 'max_features': 'auto', 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "Train Score: 0.4189609181148386\n",
      "Test Score : 0.3816854087327116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nettfa</th>\n",
       "      <td>0.701569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marr</th>\n",
       "      <td>0.248335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.022705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agesq</th>\n",
       "      <td>0.021570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fsize</th>\n",
       "      <td>0.003938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.001883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            coef\n",
       "nettfa  0.701569\n",
       "marr    0.248335\n",
       "age     0.022705\n",
       "agesq   0.021570\n",
       "fsize   0.003938\n",
       "male    0.001883"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decision tree regressor, in pipeline, with gridsearchcv\n",
    "dtr = DecisionTreeRegressor(random_state=42)\n",
    "params = {'max_depth':[2,5,8], \n",
    "          'max_features':['auto','sqrt'], \n",
    "          'min_samples_leaf':[1,5,8],\n",
    "          'min_samples_split':[2,5]}\n",
    "gs = GridSearchCV(dtr,param_grid=params,cv=5,n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best params:',gs.best_params_)\n",
    "print('Train Score:',gs.score(X_train, y_train))\n",
    "print('Test Score :',gs.score(X_test, y_test))\n",
    "dtr_best = gs.best_estimator_\n",
    "y_train_pred_dtr_best = dtr_best.predict(X_train)\n",
    "y_test_pred_dtr_best = dtr_best.predict(X_test)\n",
    "pd.DataFrame(dtr_best.feature_importances_,\n",
    "             X_train.columns, columns=['coef']).sort_values(by='coef',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than baseline score. How about a bagged decision tree regressor (multiple bootstrapped datasets, multiple trees)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_features': 1.0, 'n_estimators': 200}\n",
      "Train Score: 0.43069334199524645\n",
      "Test Score : 0.3936317689978991\n"
     ]
    }
   ],
   "source": [
    "# bagged decision tree regressor, using previously saved best estimator\n",
    "bag_dtr = BaggingRegressor(base_estimator=dtr_best) #unfixed random_state, for different trees\n",
    "params = {'max_features':[0.3,0.6,1.0],\n",
    "          'n_estimators':[50,100,200]}\n",
    "gs = GridSearchCV(bag_dtr,param_grid=params,cv=5,n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best params:',gs.best_params_)\n",
    "print('Train Score:',gs.score(X_train, y_train))\n",
    "print('Test Score :',gs.score(X_test, y_test))\n",
    "bdt_best = gs.best_estimator_\n",
    "y_train_pred_bdt_best = bdt_best.predict(X_train)\n",
    "y_test_pred_bdt_best = bdt_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better than dtr score. How about a random forest (multiple bootstrapped trees but each with random subset of features, and with multiple trees)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 8, 'max_features': 'auto', 'min_samples_leaf': 15, 'min_samples_split': 2, 'n_estimators': 400}\n",
      "Train Score: 0.47004360216977426\n",
      "Test Score : 0.4013621114566942\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nettfa</th>\n",
       "      <td>0.684071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marr</th>\n",
       "      <td>0.214252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.039628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agesq</th>\n",
       "      <td>0.038972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fsize</th>\n",
       "      <td>0.013213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.009865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            coef\n",
       "nettfa  0.684071\n",
       "marr    0.214252\n",
       "age     0.039628\n",
       "agesq   0.038972\n",
       "fsize   0.013213\n",
       "male    0.009865"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try random forest\n",
    "rfr = RandomForestRegressor(random_state=42)\n",
    "params = {'n_estimators':[200,400],\n",
    "          'max_depth':[5,8,12], \n",
    "          'max_features':['auto','sqrt'], \n",
    "          'min_samples_leaf':[8,15,20],\n",
    "          'min_samples_split':[2,5]}\n",
    "gs = GridSearchCV(rfr,param_grid=params,cv=5,n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best params:',gs.best_params_)\n",
    "print('Train Score:',gs.score(X_train, y_train))\n",
    "print('Test Score :',gs.score(X_test, y_test))\n",
    "rfr_best = gs.best_estimator_\n",
    "y_train_pred_rfr_best = rfr_best.predict(X_train)\n",
    "y_test_pred_rfr_best = rfr_best.predict(X_test)\n",
    "pd.DataFrame(rfr_best.feature_importances_,\n",
    "             X_train.columns, columns=['coef']).sort_values(by='coef',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score so far. The more trees (or 'n_estimators'), the better. How about Adaboost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.4161899307058282\n",
      "Test Score : 0.3022561276592919\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nettfa</th>\n",
       "      <td>0.670864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marr</th>\n",
       "      <td>0.114046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agesq</th>\n",
       "      <td>0.081123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.080587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fsize</th>\n",
       "      <td>0.036797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.016583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            coef\n",
       "nettfa  0.670864\n",
       "marr    0.114046\n",
       "agesq   0.081123\n",
       "age     0.080587\n",
       "fsize   0.036797\n",
       "male    0.016583"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the best estimator so far: rfr_best\n",
    "ada = AdaBoostRegressor(rfr_best)\n",
    "ada.fit(X_train,y_train)\n",
    "print('Train Score:',ada.score(X_train, y_train))\n",
    "print('Test Score :',ada.score(X_test, y_test))\n",
    "y_train_pred_ada = ada.predict(X_train)\n",
    "y_test_pred_ada = ada.predict(X_test)\n",
    "pd.DataFrame(ada.feature_importances_,\n",
    "             X_train.columns, columns=['coef']).sort_values(by='coef',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than linreg and knn and baseline methods, but worse than decision trees/bagged decision trees/random forest methods. How about SVR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.11967993270565624\n",
      "Test Score : 0.07218599634463763\n"
     ]
    }
   ],
   "source": [
    "# not using gridsearch here, it takes too long\n",
    "svr = SVR()\n",
    "svr.fit(X_train,y_train)\n",
    "print('Train Score:',svr.score(X_train, y_train))\n",
    "print('Test Score :',svr.score(X_test, y_test))\n",
    "y_train_pred_svr = svr.predict(X_train)\n",
    "y_test_pred_svr = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A:\n",
    "Bootstrapping is any test or metric that relies on random sampling with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A:\n",
    "Decision tree: '**single set of data**, running through a **single decision tree**'. Accuracy score is solely from this 1 tree's predictions.\n",
    "\n",
    "Bagged decision tree: '**multiple sets of data** obtained by bootstrapping (sampling with replacement) the original dataset, each running through **different decision trees (ensemble method)**'. Accuracy score is the aggregate mean of all decision trees' predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A:\n",
    "Bagged decision tree: At each node, **ALL** features are considered before splitting at the node.\n",
    "\n",
    "Random forest: At each node, only a **RANDOM SUBSET** of features (p/3 for regressor trees, sqrt p for classification trees) are considered before splitting at the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A:\n",
    "Random forests result in trees each ending up with different features after every split at every node, thereby resulting in decorrelated trees and more reliable average output. Hence random forests are more robust against correlated features. The bias is slightly higher, but the variance is greatly reduced, hence overfitting is greatly reduced too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linreg (train)              : 20.164244947447397\n",
      "ridgeCV (train)             : 20.16424496318116\n",
      "lassoCV (train)             : 20.57350567488912\n",
      "knn (train)                 : 18.79401829908604\n",
      "decision tree (train)       : 18.274452949514924\n",
      "bagged decision tree (train): 18.089011884460717\n",
      "random forest (train)       : 17.45266619805475\n",
      "adaboost (train)            : 18.317976741967485\n",
      "svr (train)                 : 22.493757748703892\n",
      "\n",
      "linreg (test)               : 20.89741661081878 , Difference of 0.7331716633713832 from train\n",
      "ridgeCV (test)              : 20.89741963092309 , Difference of 0.7331746677419275 from train\n",
      "lassoCV (test)              : 21.331991566007936 , Difference of 0.7584858911188164 from train\n",
      "knn (test)                  : 20.838822450594034 , Difference of 2.044804151507993 from train\n",
      "decision tree (test)        : 19.298024189703625 , Difference of 1.0235712401887014 from train\n",
      "bagged decision tree (test) : 19.110687836769625 , Difference of 1.0216759523089074 from train\n",
      "random forest (test)        : 18.988479891336095 , Difference of 1.5358136932813444 from train\n",
      "adaboost (test)             : 20.500106363775057 , Difference of 2.1821296218075723 from train\n",
      "svr (test)                  : 23.639514904981482 , Difference of 1.1457571562775897 from train\n"
     ]
    }
   ],
   "source": [
    "print('linreg (train)              :',mean_squared_error(y_train, y_train_pred_linreg)**0.5)\n",
    "print('ridgeCV (train)             :',mean_squared_error(y_train, y_train_pred_ridgecv)**0.5)\n",
    "print('lassoCV (train)             :',mean_squared_error(y_train, y_train_pred_lassocv)**0.5)\n",
    "print('knn (train)                 :',mean_squared_error(y_train, y_train_pred_knn_best)**0.5)\n",
    "print('decision tree (train)       :',mean_squared_error(y_train, y_train_pred_dtr_best)**0.5)\n",
    "print('bagged decision tree (train):',mean_squared_error(y_train, y_train_pred_bdt_best)**0.5)\n",
    "print('random forest (train)       :',mean_squared_error(y_train, y_train_pred_rfr_best)**0.5)\n",
    "print('adaboost (train)            :',mean_squared_error(y_train, y_train_pred_ada)**0.5)\n",
    "print('svr (train)                 :',mean_squared_error(y_train, y_train_pred_svr)**0.5)\n",
    "\n",
    "print('\\nlinreg (test)               :',mean_squared_error(y_test, y_test_pred_linreg)**0.5,\n",
    "     ', Difference of',mean_squared_error(y_test, y_test_pred_linreg)**0.5-mean_squared_error(y_train, y_train_pred_linreg)**0.5,'from train')\n",
    "print('ridgeCV (test)              :',mean_squared_error(y_test, y_test_pred_ridgecv)**0.5,\n",
    "     ', Difference of',mean_squared_error(y_test, y_test_pred_ridgecv)**0.5-mean_squared_error(y_train, y_train_pred_ridgecv)**0.5,'from train')\n",
    "print('lassoCV (test)              :',mean_squared_error(y_test, y_test_pred_lassocv)**0.5,\n",
    "     ', Difference of',mean_squared_error(y_test, y_test_pred_lassocv)**0.5-mean_squared_error(y_train, y_train_pred_lassocv)**0.5,'from train')\n",
    "print('knn (test)                  :',mean_squared_error(y_test, y_test_pred_knn_best)**0.5,\n",
    "     ', Difference of',mean_squared_error(y_test, y_test_pred_knn_best)**0.5-mean_squared_error(y_train, y_train_pred_knn_best)**0.5,'from train')\n",
    "print('decision tree (test)        :',mean_squared_error(y_test, y_test_pred_dtr_best)**0.5,\n",
    "     ', Difference of',mean_squared_error(y_test, y_test_pred_dtr_best)**0.5-mean_squared_error(y_train, y_train_pred_dtr_best)**0.5,'from train')\n",
    "print('bagged decision tree (test) :',mean_squared_error(y_test, y_test_pred_bdt_best)**0.5,\n",
    "     ', Difference of',mean_squared_error(y_test, y_test_pred_bdt_best)**0.5-mean_squared_error(y_train, y_train_pred_bdt_best)**0.5,'from train')\n",
    "print('random forest (test)        :',mean_squared_error(y_test, y_test_pred_rfr_best)**0.5,\n",
    "     ', Difference of',mean_squared_error(y_test, y_test_pred_rfr_best)**0.5-mean_squared_error(y_train, y_train_pred_rfr_best)**0.5,'from train')\n",
    "print('adaboost (test)             :',mean_squared_error(y_test, y_test_pred_ada)**0.5,\n",
    "     ', Difference of',mean_squared_error(y_test, y_test_pred_ada)**0.5-mean_squared_error(y_train, y_train_pred_ada)**0.5,'from train')\n",
    "print('svr (test)                  :',mean_squared_error(y_test, y_test_pred_svr)**0.5,\n",
    "     ', Difference of',mean_squared_error(y_test, y_test_pred_svr)**0.5-mean_squared_error(y_train, y_train_pred_svr)**0.5,'from train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A:\n",
    "\n",
    "Perhaps KNN and AdaBoost had the greatest signs of overfitting, as their test RMSE scores were larger than their train RMSE scores by the largest margins (~+2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A:\n",
    "I would pick the random forest method, as it yields the lowest RMSE (18.99), and lowest R2 score of 0.40 as seen earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A:\n",
    "1. From the optimum gridsearchcv parameters, I would choose a tighter range of values around the best hyperparameters, to gridsearch over. \n",
    "2. try out the extra trees regressor (no bootstrapping of original dataset), which might yield better results. This is because splits are done at random at each node, instead of choosing splits based on the greatest drop in Gini impurity. Results may be better with more diversified trees. Also, the 'extra trees' works better when there are a few main predictors, and many irrelevant ones (https://www.thekerneltrip.com/statistics/random-forest-vs-extra-tree/#:~:targetText=The%20main%20difference%20between%20random,(for%20the%20extra%20trees)), which might be the case for this problem - we saw earlier from the linear and decision tree/ensemble methods, that they typically have 2 stronger predictors (high coefficients) and the rest were much weaker (lower coefficients). \n",
    "3. try Gradient Boosting, or XG Boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.4604933550646788\n",
      "Test Score : 0.41431707220450387\n"
     ]
    }
   ],
   "source": [
    "# gradient boosting\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train,y_train)\n",
    "print('Train Score:',gbr.score(X_train,y_train))\n",
    "print('Test Score :',gbr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:36:28] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Train Score: 0.4557327718993076\n",
      "Test Score : 0.4133378681972416\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEWCAYAAACjYXoKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYFIWdxvHvK4cCoxACIqKIBBRQYBSPuBoyhGC8j1UxLFnBY02CMZpVV5JsPDaJ4hUlJLtEPFDjKl5LjBqPgLO6xARBEfAYz3FFDeAKERDl+u0fXYPtOMO01PR0F7yf5+mHurr67dKZd+roakUEZmZmaWxT6gBmZpZ9LhMzM0vNZWJmZqm5TMzMLDWXiZmZpeYyMTOz1FwmZkUmabKkn5Q6h1kxyZ8zsXIlqRboBqzPm7xHRLyTYp1VwG8jYpd06bJJ0lRgUUT8a6mz2JbFeyZW7o6OiIq8x2YXSXOQ1LqUr5+GpFalzmBbLpeJZZKkL0v6k6Tlkp5L9jjq5p0q6UVJKyS9LunbyfQOwB+AnSWtTB47S5oq6Wd5z6+StChvvFbShZLmA6sktU6ed6+kpZLekPT9TWTduP66dUv6F0lLJL0r6ThJR0h6WdL7kn6U99xLJN0jaVryfp6RNDhvfn9J1cl2eF7SMfVe9z8kPSRpFXA6MBr4l+S9/z5Zbryk15L1vyDp+Lx1jJX0P5KulrQsea+H583vLOlmSe8k86fnzTtK0rwk258kDSr4P7BljsvEMkdSD+BB4GdAZ+B84F5JXZNFlgBHATsApwLXSto3IlYBhwPvbMaezijgSKATsAH4PfAc0AMYDpwr6RsFrmsnYLvkuRcBU4BvAUOArwAXSeqdt/yxwN3Je/1PYLqkNpLaJDkeBXYEzgZul7Rn3nP/Afg5sD1wK3A7cGXy3o9Olnkted2OwKXAbyV1z1vHgUAN0AW4ErhRkpJ5twHtgb2SDNcCSNoXuAn4NvBF4DfA/ZK2LXAbWca4TKzcTU/+sl2e91fvt4CHIuKhiNgQEY8Bc4AjACLiwYh4LXL+m9wv26+kzPHLiHgrIlYD+wNdI+LfImJNRLxOrhC+WeC61gI/j4i1wJ3kfklPjIgVEfE88DyQ/1f83Ii4J1n+F+SK6MvJowKYkOSYCTxArvjq/C4iZiXb6aOGwkTE3RHxTrLMNOAV4IC8Rd6MiCkRsR64BegOdEsK53DgOxGxLCLWJtsb4J+A30TEXyJifUTcAnycZLYtUGaP/9pW47iI+GO9absBJ0k6Om9aG+BxgOQwzMXAHuT+YGoPLEiZ4616r7+zpOV501oBTxa4rv9LfjEDrE7+XZw3fzW5kvjMa0fEhuQQ3M518yJiQ96yb5Lb42kod4MknQL8M9ArmVRBruDq/DXv9T9MdkoqyO0pvR8RyxpY7W7AGEln501rm5fbtjAuE8uit4DbIuKf6s9IDqPcC5xC7q/ytckeTd1hmYYuX1xFrnDq7NTAMvnPewt4IyL6bk74zbBr3YCkbYBdgLrDc7tK2iavUHoCL+c9t/77/dS4pN3I7VUNB56KiPWS5vHJ9tqUt4DOkjpFxPIG5v08In5ewHpsC+DDXJZFvwWOlvQNSa0kbZec2N6F3F+/2wJLgXXJXsqhec9dDHxRUse8afOAI5KTyTsB5zbx+rOBD5KT8u2SDHtL2r/Z3uGnDZH098mVZOeSO1z0Z+Av5IrwX5JzKFXA0eQOnTVmMZB/PqYDuYJZCrmLF4C9CwkVEe+Su6Dh3yV9IckwNJk9BfiOpAOV00HSkZK2L/A9W8a4TCxzIuItcielf0Tul+BbwAXANhGxAvg+cBewjNwJ6PvznvsScAfwenIeZmdyJ5GfA2rJnV+Z1sTrryf3S7sSeAN4D7iB3AnsYvgdcDK59/OPwN8n5yfWAMeQO2/xHvDvwCnJe2zMjcCAunNQEfECcA3wFLmiGQjM+hzZ/pHcOaCXyF34cC5ARMwhd97kV0nuV4Gxn2O9ljH+0KJZGZN0CdAnIr5V6ixmm+I9EzMzS81lYmZmqfkwl5mZpeY9EzMzS22r+ZxJp06dok+fPqWOUZBVq1bRoUOHUsdoknM2v6xkzUpOyE7Wcs05d+7c9yKia1PLbTVl0q1bN+bMmVPqGAWprq6mqqqq1DGa5JzNLytZs5ITspO1XHNKerOQ5XyYy8zMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzy4iPPvqIAw44gMGDB7PXXntx8cUXAzB27Fh23313KisrqaysZN68eQAsW7aM448/nkGDBnHAAQewcOHComVTRBRt5Z+HpOOAlyPihWR8LPBoRLyTjH8FmAysBQ6KiNWfZ/09e/eJbUZObN7QRXLewHVcs6B1qWM0yTmbX1ayZiUnZCdrUzlrJxxJRLBq1SoqKipYu3YthxxyCBMnTmTy5MkcddRRnHjiiZ96zgUXXEBFRQUXX3wxL730EmeddRYzZsz4XLkkzY2I/Zparpz2TI4DBuSNjwV2zhsfDVwdEZWft0jMzLYEkqioqABg7dq1rF27FkmNLv/CCy8wfPhwAPr160dtbS2LFy8uSrailYmkXpJelDRF0vOSHpXUTtKXJD0saa6kJyX1k/R3wDHAVZLmSboQ2A+4PRk/GxgJXCTpdkkVkmZIekbSAknHFut9mJmVk/Xr11NZWcmOO+7IiBEjOPDAAwH48Y9/zKBBg/jBD37Axx9/DMDgwYO57777AJg9ezZvvvkmixYtKkquYu+Z9AV+HRF7AcuBE4DrgbMjYghwPvDvEfEn4H7ggmTP4wpgDjA6GZ+UN3808BFwfETsCwwDrtGm6tnMbAvRqlUr5s2bx6JFi5g9ezYLFy7k8ssv56WXXuLpp5/m/fff54orrgBg/PjxLFu2jMrKSiZNmsQ+++xD69bFOeRX7AOJb0TEvGR4LtAL+Dvg7rzf/dtuxnoFXCZpKLAB6AF0A/76qYWkM4EzAbp06cpFA9dtxku1vG7tcsdPy51zNr+sZM1KTshO1qZyVldXf2Zar169+PWvf83JJ59MTU0NAPvssw/Tpk1j6NChAIwZM4YxY8YQEYwaNYpFixaxbNmyZs9f7DL5OG94Pblf+MsjojLlekcDXYEhEbFWUi2wXf2FIuJ6cntC9OzdJ7JwEg62nBOG5SIrOSE7WbOSE7KTtckT8KOrWLp0KW3atKFTp06sXr2an/zkJ1x44YXsueeedO/enYhg+vTpfPWrX6Wqqorly5fTvn172rZty5QpUzj00EM58sgji5K/pbfwB8Abkk6KiLuTQ1ODIuI5YAWwfd6y9cfzdQSWJEUyDNitqKnNzMrAu+++y5gxY1i/fj0bNmxg5MiRHHXUUXzta19j6dKlRASVlZVMnjwZgBdffJFTTjmFVq1aMWDAAG688cbihYuIojzIHdJamDd+PnAJsDvwMPAc8AJwUTL/4GT8WeBL5M6v1ADzgHbAVODEZNkuwFPkzqvcALwI9NpUnj322COy4vHHHy91hII4Z/PLStas5IzITtZyzQnMiQJ+5xdtzyQiaoG988avzpt9WAPLz+LTlwa/BtybNz42b9n3gIOaKaqZmaVUTp8zMTOzjHKZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLrXWpA7SU1WvX02v8g6WOUZDzBq5jbAayZiXn1MM6cNppp/HAAw+w4447snDhwk/Nv/rqq7ngggtYunQpXbp0YdmyZZx22mm89tprbLfddtx0003svffeJUpvlg3eM7GtwtixY3n44Yc/M/2tt97iscceo2fPnhunXXbZZVRWVjJ//nxuvfVWzjnnnJaMapZJLhPbKgwdOpTOnTt/ZvoPfvADrrzySiRtnPbCCy8wfPhwAPr160dtbS2LFy9usaxmWVQ2ZSJpuqS5kp6XdGYy7XRJL0uqljRF0q+S6V0l3Svp6eRxcGnTWxbdf//99OjRg8GDB39q+uDBg7nvvvsAmD17Nm+++SaLFi0qRUSzzCincyanRcT7ktoBT0t6EPgJsC+wApgJPJcsOxG4NiL+R1JP4BGgf/0VJqV0JkCXLl25aOC6Fngb6XVrlzsfUe6yknPlypVUV1fz17/+lVWrVlFdXc1HH33EhRdeyFVXXbVxfNasWXTs2JGDDz6YX/3qV/Tp04fevXvTp08fnn32WVasWNFiWctdVnJCdrJmJWdjFBGlzgCApEuA45PRXsDlQP+IGJPM/z6wR0R8T9IS4J28p3cF+kVEoz/tPXv3iW1GTixG9GZ33sB1XLOgnHq+YVnJOfWwDlRVVVFbW8tRRx3FwoULWbBgAcOHD6d9+/YALFq0iJ133pnZs2ez0047bXxuRLD77rszf/58dthhh6Jnra6upqqqquivk1ZWckJ2spZrTklzI2K/ppYri98EkqqArwMHRcSHkqqBGhrY20hskyy7umUS2pZm4MCBLFmyZON4r169mDNnDl26dGH58uW0b9+etm3bcsMNNzB06NAWKRKzLCuLMgE6AsuSIukHfBmYAnxV0hfIHeY6AViQLP8o8D3gKgBJlRExb1Mv0K5NK2omHFms/M2qurqa2tFVpY7RpCzlHDVqFNXV1bz33nvssssuXHrppZx++ukNLv/iiy9yyimn0KpVKwYMGMCNN97YwonNsqdcyuRh4DuS5pPbI/kz8DZwGfAXcoe0XgD+liz/feDXyfKtgSeA77R0aMuOO+64Y5Pza2trNw4fdNBBvPLKK0VOZLZlKYsyiYiPgcPrT5c0JyKul9Qa+C9yeyRExHvAyS2b0szMGlM2lwY34hJJ84CFwBvA9BLnMTOzBpTFnkljIuL8UmcwM7OmlfueiZmZZYDLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmltrnLhNJX5A0qBhhzMwsmwoqk+Rrc3eQ1Jnctx3eLOkXxY1mZmZZUeieSceI+AD4e+DmiBhC7suszMzMCi6T1pK6AyOBB4qYx8zMMqjQMvk34BHgtYh4WlJvwN8eZGZmQIG3oI+Iu4G788ZfJ/c1umZmZgWfgN9D0gxJC5PxQZL+tbjRzMwsKwo9zDUF+CGwFiAi5gPfLFYoMzPLlkLLpH1EzK43bV1zhzEzs2wqtEzek/QlIAAknQi8W7RUZmaWKYV+B/xZwPVAP0lvA28Ao4uWyszMMqXJMpG0DbBfRHxdUgdgm4hYUfxoZmaWFU0e5oqIDcD3kuFVLhIzM6uv0HMmj0k6X9KukjrXPYqazMzMMqPQcyanJf+elTctgN7NG8fMzLKo0E/A717sIMW2eu16eo1/sNQxCnLewHWM3YystROO3Di8fPlyzjjjDBYuXIgkbrrpJq677jpqamo2zu/UqRPz5s1rttxmtvUqqEwkndLQ9Ii4tXnjWHM555xzOOyww7jnnntYs2YNH374IdOmTds4/7zzzqNjx44lTGhmW5JCD3Ptnze8HTAceAYoWZlIah0R6/LGW0XE+lLlKScffPABTzzxBFOnTgWgbdu2tG3bduP8iOCuu+5i5syZJUpoZluaQg9znZ0/LqkjcNvmvKCkXsDDwP8AXyb5si3gUmBHPvn8ynVAO2A1cGpE1EgaCxxJrtA6SPo34GJyH6CsBAZsTqYtzeuvv07Xrl059dRTee655xgyZAgTJ06kQ4cOADz55JN069aNvn37ljipmW0pFBGf/0lSG2B+RPTfjOf2Al4F9gGeB54mVyinA8cApwKnAB9GxDpJXwe+GxEnJGXyM2BQRLwvqQp4ENg7It5o4LXOBM4E6NKl65CLrpvyeeOWRLd2sHj153/ewB65w1Y1NTWMGzeOSZMmMWDAACZNmkSHDh047bTcdRTXXnstPXr0YOTIkalyrly5koqKilTraAlZyQnZyZqVnJCdrOWac9iwYXMjYr+mliv0nMnvSW6lQu5y4gHk3ZJ+M7wREQuSdT8PzIiIkLQA6AV0BG6R1Dd53TZ5z30sIt7PG5/dUJEARMT15D65T8/efeKaBYUe1Sut8wauY3Oy1o6uAqBfv35cfvnljBs3DoBWrVoxYcIEqqqqWLduHSeffDJz585ll112SZWzurqaqqqqVOtoCVnJCdnJmpWckJ2sWcnZmEJ/Y12dN7wOeDMiFqV43Y/zhjfkjW9IMv0UeDwijk/2ZKrzll9Vb131x7d6O+20E7vuuis1NTXsueeezJgxgwEDckcA//jHP9KvX7/URWJmlq/QMjkiIi7MnyDpivrTmlFH4O1keGyRXmOLNmnSJEaPHs2aNWvo3bs3N998MwB33nkno0aNKnE6M9vSFFomI4D6xXF4A9Oay5XkDnP9M9Aslxy1a9OKmrzPYZSz6urqjYesNldlZSVz5sz5zPS6K7zMzJrTJstE0neBcUBvSfPzZm0PzNqcF4yIWmDvvPGxjczbI+9pP0nmTwWm5i1fzacPgZmZWQk0tWfyn8AfgMuB8XnTV9Q7CW5mZluxTZZJRPwN+BswCkDSjuQ+41EhqSIi/rf4Ec3MrNwVdNdgSUdLeoXcl2L9N1BLbo/FzMys4FvQ/4zcp9VfTm76OJzNPGdiZmZbnkLLZG1E/B+wjaRtIuJxcrcvMTMzK/jS4OWSKoAngdslLSH34UUzM7OC90yOBT4EziV3k8bXgKOLFcrMzLKl0LsGr5K0G9A3Im6R1B5oVdxoZmaWFYVezfVPwD3Ab5JJPYDpxQplZmbZUuhhrrOAg4EPACLiFXLfPWJmZlZwmXwcEWvqRiS15pNb0puZ2Vau0DL5b0k/AtpJGkHuu0x+X7xYZmaWJYWWyXhgKbAA+DbwEPCvxQplZmbZ0tRdg3tGxP9GxAZgSvIwMzP7lKb2TDZesSXp3iJnMTOzjGqqTJQ33LuYQczMLLuaKpNoZNjMzGyjpj4BP1jSB+T2UNolwyTjERE7FDWdmZllQlNfjuVbppiZWZMKvTTYzMysUS4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLJOOWL1/OiSeeSL9+/ejfvz9PPfUU77//PiNGjKBv376MGDGCZcuWlTqmmW3hCvoO+OYm6fvAd4FnImJ0vXk7A7+MiBOb8zVXr11Pr/EPNucqi+a8gesYu4mstROO3Dh8zjnncNhhh3HPPfewZs0aPvzwQy677DKGDx/O+PHjmTBhAhMmTOCKK65oiehmtpUq1Z7JOOCI+kUCEBHvNHeRbKk++OADnnjiCU4//XQA2rZtS6dOnfjd737HmDFjABgzZgzTp0/f1GrMzFJr8TKRNJncHYjvl3SxpHnJ41lJ20vqJWlhsuwNefOXSro4mX6BpKclzZd0aUu/h3Lx+uuv07VrV0499VT22WcfzjjjDFatWsXixYvp3r07AN27d2fJkiUlTmpmWzpFtPzNgCXVAvsBNwMTImKWpArgI2AX4IGI2Dtv+d2AR4BvAHsCJ5L7xkcB9wNXRsQTDbzOmcCZAF26dB1y0XXZ+G6vbu1g8erG5w/s0RGAmpoaxo0bx6RJkxgwYACTJk2iQ4cO3HfffTzwwAMblz/66KP5/e+b/1uWV65cSUVFRbOvt7llJSdkJ2tWckJ2spZrzmHDhs2NiP2aWq4k50zyzAJ+Iel24L6IWCTpUwtI2o7cd85/LyLelHQ2cCjwbLJIBdAX+EyZRMT1wPUAPXv3iWsWlPrtFua8gevYVNba0VUA9OvXj8svv5xx48YB0KpVKyZMmECPHj3Yc8896d69O++++y4777wzVVVVzZ6zurq6KOttblnJCdnJmpWckJ2sWcnZmJJezRURE4AzgHbAnyX1a2CxyeSK5o/JuIDLI6IyefSJiBtbKHJZ2Wmnndh1112pqakBYMaMGQwYMIBjjjmGW265BYBbbrmFY489tpQxzWwrUNI/1SV9KSIWAAskHQT0A+blzT8L2D4pnTqPAD+VdHtErJTUA1gbEVvliYFJkyYxevRo1qxZQ+/evbn55pvZsGEDI0eO5MYbb6Rnz57cfffdpY5pZlu4Uh/3OVfSMGA98ALwB6B73vzzgbWS6gpmckRMltQfeCo5JLYS+BawyTJp16YVNXmX1Jaz6urqjYeymlJZWcmcOXM+M33GjBnNnMrMrHElKZOI6JUMnt3A7Fpg72S53Rt5/kRgYjGymZnZ5+dPwJuZWWouEzMzS81lYmZmqblMzMwsNZeJmZml5jIxM7PUXCZmZpaay8TMzFJzmZiZWWouEzMzS81lYmZmqblMzMwsNZeJmZml5jIxM7PUXCZmZpaay8TMzFJzmZiZWWouEzMzS81lYmZmqblMzMwsNZeJmZml5jIxM7PUXCZmZpaay8TMzFJzmZiZWWouEzMzS81l0ozeeusthg0bRv/+/dlrr72YOHEiACeffDKVlZVUVlbSq1cvKisrS5zUzKx5tS51gEJJqgLOj4ijNuf5q9eup9f4B5s3VKJ2wpEAtG7dmmuuuYZ9992XFStWMGTIEEaMGMG0adM2LnveeefRsWPHouQwMyuVzJRJFnTv3p3u3bsDsP3229O/f3/efvttBgwYAEBEcNdddzFz5sxSxjQza3YtephLUi9JL0m6QdJCSbdL+rqkWZJekXRA8viTpGeTf/dsYD0dJN0k6elkuWNb8n0Uora2lmeffZYDDzxw47Qnn3ySbt260bdv3xImMzNrfqU4Z9IHmAgMAvoB/wAcApwP/Ah4CRgaEfsAFwGXNbCOHwMzI2J/YBhwlaQOLZC9ICtXruSEE07guuuuY4cddtg4/Y477mDUqFElTGZmVhyKiJZ7MakX8FhE9E3GbwUeiYjbJfUG7gOOBn4J9AUCaBMR/fLPmUiaA2wHrEtW3Rn4RkS8WO/1zgTOBOjSpeuQi66bUpT3NbDHJ+dA1q1bxw9/+EP2339/Ro4cuXH6+vXrOemkk/jNb35D165dN7m+lStXUlFRUZSszck5m19WsmYlJ2Qna7nmHDZs2NyI2K+p5UpxzuTjvOENeeMbyOX5KfB4RByflE91A+sQcEJE1GzqhSLieuB6gJ69+8Q1C4rzdmtHV9W9HmPGjOHggw/muuuu+9QyDz/8MAMHDuSkk05qcn3V1dVUVVUVIWnzcs7ml5WsWckJ2cmalZyNKcdLgzsCbyfDYxtZ5hHgbEkCkLRPC+Rq0qxZs7jtttuYOXPmxkuBH3roIQDuvPNOH+Iysy1WOV7NdSVwi6R/Bhq77OmnwHXA/KRQaoFNXjLcrk0rapJLeIvlkEMOobHDhlOnTi3qa5uZlVKLlklE1AJ7542PbWTeHnlP+0kyv5rkkFdErAa+XcSoZmb2OZTjYS4zM8sYl4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCw1l4mZmaXmMjEzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLTRFR6gwtQtIKoKbUOQrUBXiv1CEK4JzNLytZs5ITspO1XHPuFhFdm1qodUskKRM1EbFfqUMUQtKcLGR1zuaXlaxZyQnZyZqVnI3xYS4zM0vNZWJmZqltTWVyfakDfA5ZyeqczS8rWbOSE7KTNSs5G7TVnIA3M7Pi2Zr2TMzMrEhcJmZmltpWUSaSDpNUI+lVSeNLnSefpFpJCyTNkzQnmdZZ0mOSXkn+/UKJst0kaYmkhXnTGsymnF8m23i+pH1LnPMSSW8n23WepCPy5v0wyVkj6RstmHNXSY9LelHS85LOSaaX1TbdRM5y3KbbSZot6bkk66XJ9N0l/SXZptMktU2mb5uMv5rM71XinFMlvZG3TSuT6SX7edpsEbFFP4BWwGtAb6At8BwwoNS58vLVAl3qTbsSGJ8MjweuKFG2ocC+wMKmsgFHAH8ABHwZ+EuJc14CnN/AsgOS/we2BXZP/t9o1UI5uwP7JsPbAy8necpqm24iZzluUwEVyXAb4C/JtroL+GYyfTLw3WR4HDA5Gf4mMK3EOacCJzawfMl+njb3sTXsmRwAvBoRr0fEGuBO4NgSZ2rKscAtyfAtwHGlCBERTwDv15vcWLZjgVsj589AJ0ndS5izMccCd0bExxHxBvAquf9Hii4i3o2IZ5LhFcCLQA/KbJtuImdjSrlNIyJWJqNtkkcAXwPuSabX36Z12/oeYLgklTBnY0r287S5toYy6QG8lTe+iE3/YLS0AB6VNFfSmcm0bhHxLuR+sIEdS5busxrLVo7b+XvJIYKb8g4VlkXO5PDKPuT+Qi3bbVovJ5ThNpXUStI8YAnwGLk9o+URsa6BPBuzJvP/BnyxFDkjom6b/jzZptdK2rZ+zkQ5/Dxt0tZQJg391VFO10MfHBH7AocDZ0kaWupAm6nctvN/AF8CKoF3gWuS6SXPKakCuBc4NyI+2NSiDUxrsawN5CzLbRoR6yOiEtiF3B5R/03kKVnW+jkl7Q38EOgH7A90Bi4sdc7NtTWUySJg17zxXYB3SpTlMyLineTfJcB/kfthWFy3S5v8u6R0CT+jsWxltZ0jYnHyw7sBmMInh11KmlNSG3K/oG+PiPuSyWW3TRvKWa7btE5ELAeqyZ1j6CSp7t6D+Xk2Zk3md6TwQ6TNnfOw5JBiRMTHwM2U2Tb9PLaGMnka6Jtc3dGW3Em3+0ucCQBJHSRtXzclRZ+DAAAC/klEQVQMHAosJJdvTLLYGOB3pUnYoMay3Q+cklyF8mXgb3WHbkqh3vHl48ltV8jl/GZyVc/uQF9gdgtlEnAj8GJE/CJvVllt08Zyluk27SqpUzLcDvg6uXM8jwMnJovV36Z12/pEYGYkZ7xLkPOlvD8iRO68Tv42LZufp4KU+gqAlniQuzLiZXLHUn9c6jx5uXqTuwrmOeD5umzkjuHOAF5J/u1conx3kDucsZbcX0qnN5aN3G75r5NtvADYr8Q5b0tyzCf3g9k9b/kfJzlrgMNbMOch5A5VzAfmJY8jym2bbiJnOW7TQcCzSaaFwEXJ9N7kCu1V4G5g22T6dsn4q8n83iXOOTPZpguB3/LJFV8l+3na3Idvp2JmZqltDYe5zMysyFwmZmaWmsvEzMxSc5mYmVlqLhMzM0utddOLmNmmSFpP7vLNOsdFRG2J4piVhC8NNktJ0sqIqGjB12sdn9x3yqws+DCXWZFJ6i7pieT7KhZK+koy/TBJzyTfcTEjmdZZ0vTkxn9/ljQomX6JpOslPQrcmtw08CpJTyfLfruEb9HMh7nMmkG75G6wAG9ExPH15v8D8EhE/FxSK6C9pK7k7m81NCLekNQ5WfZS4NmIOE7S14Bbyd1YEWAIcEhErE7uMP23iNg/udPsLEmPRu4W8GYtzmVilt7qyN0NtjFPAzclN0+cHhHzJFUBT9T98o+IupsNHgKckEybKemLkjom8+6PiNXJ8KHAIEl195/qSO6eWC4TKwmXiVmRRcQTyVcLHAncJukqYDkN31J8U7ceX1VvubMj4pFmDWu2mXzOxKzIJO0GLImIKeTuxrsv8BTw1eQuu+Qd5noCGJ1MqwLei4a/8+QR4LvJ3g6S9kjuPG1WEt4zMSu+KuACSWuBlcApEbE0Oe9xn6RtyH2HyQhy37N+s6T5wId8crv0+m4AegHPJLcvX0qJvt7ZDHxpsJmZNQMf5jIzs9RcJmZmlprLxMzMUnOZmJlZai4TMzNLzWViZmapuUzMzCy1/wft+dYSIIRW/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  xgboost\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.fit(X_train,y_train)\n",
    "print('Train Score:',xgbr.score(X_train,y_train))\n",
    "print('Test Score :',xgbr.score(X_test,y_test))\n",
    "\n",
    "# see importance of different features\n",
    "xgb.plot_importance(xgbr)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inc',\n",
       " 'marr',\n",
       " 'male',\n",
       " 'age',\n",
       " 'fsize',\n",
       " 'nettfa',\n",
       " 'p401k',\n",
       " 'pira',\n",
       " 'incsq',\n",
       " 'agesq']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [col for col in df.columns if col!='e401k']\n",
    "# X = df.drop(columns=['e401k','p401k','pira','inc','incsq'])\n",
    "# y = df['inc']\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
