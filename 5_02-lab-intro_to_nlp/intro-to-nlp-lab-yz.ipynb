{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Natural Language Processing Lab\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, we'll explore scikit-learn and NLTK's capabilities for processing text even further. We'll use the 20 newsgroups data set, which is provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import vectorizer, tokenizer, stemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# others\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the scikit-learn data set:\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use the `fetch_20newsgroups` function to download a training and testing set.\n",
    "\n",
    "The \"20 Newsgroups\" dataset is described [here](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html).\n",
    "\n",
    "For this lab let's choose 4 categories to analyze.  The full list is given below.\n",
    "\n",
    "\n",
    "```python\n",
    "['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "```\n",
    "\n",
    "Note that the solution code will use these categories:\n",
    "- `alt.atheism`\n",
    "- `talk.religion.misc`\n",
    "- `comp.graphics`\n",
    "- `sci.space`\n",
    "\n",
    "Also remove the headers, footers, and quotes using the `remove` keyword argument of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Information from the Data's Dictionary format \n",
    "\n",
    "categories = ['alt.atheism','talk.religion.misc','comp.graphics','sci.space']  # Fill in whatever categories you want to use!!\n",
    "\n",
    "# Setting out training data\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "# Setting our testing data\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What does the `shuffle` argument do?  Why are we setting a `random_state`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:\n",
    "\n",
    "Shuffling is important for methods (which we might use later on) which make the assumption that the samples are independent and identically distributed (i.i.d.), such as stochastic gradient descent.\n",
    "\n",
    "We set a random_state, so as to fix the sampling of data rows from the train and test dataset, so that the sampled results are replicable/reproducible everytime the code is run. This reproducibility allows us to troubleshoot our model more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Inspect the data.\n",
    "\n",
    "We've downloaded a few `newsgroups` categories and removed their headers, footers, and quotes.\n",
    "\n",
    "Because this is a scikit-learn data set, it comes with pre-split training and testing sets (note: we were able to call \"train\" and \"test\" in subset).\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1) What data type is `data_train`?\n",
    "- Is it a list? A dictionary? What else?\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point. What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train; #loooooong printout surpressed, but this line would show the entire data structure\n",
    "            #and I've seen that its a dictionary of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['data']; #loooooong printout surpressed, but this line would show how ...['data'] looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Dell\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38816'\n",
      " 'C:\\\\Users\\\\Dell\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\talk.religion.misc\\\\83741'\n",
      " 'C:\\\\Users\\\\Dell\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\sci.space\\\\61092'\n",
      " ...\n",
      " 'C:\\\\Users\\\\Dell\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38737'\n",
      " 'C:\\\\Users\\\\Dell\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\alt.atheism\\\\53237'\n",
      " 'C:\\\\Users\\\\Dell\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38269']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('<U94')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_train['filenames'])\n",
    "data_train['filenames'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(data_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _20newsgroups_dataset:\\n\\nThe 20 newsgroups text dataset\\n------------------------------\\n\\nThe 20 new'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['DESCR'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A:\n",
    "\n",
    "1. As seen above, data_train is a dictionary (due to the {}) of lists of unicode/strings/integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qty of data        : 2034\n",
      "Qty of filenames   : 2034\n",
      "Qty of target_names: 4\n",
      "Qty of target      : 2034\n",
      "Qty of DESCR       : 9535\n"
     ]
    }
   ],
   "source": [
    "print('Qty of data        :',len(data_train['data']))\n",
    "print('Qty of filenames   :',len(data_train['filenames']))\n",
    "print('Qty of target_names:',len(data_train['target_names']))    #only 4 because we only selected 4 categories here\n",
    "print('Qty of target      :',len(data_train['target']))\n",
    "print('Qty of DESCR       :',len(data_train['DESCR']))           #>2034 because this is just a string of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A:\n",
    "\n",
    "2. As seen above, data_train has 2034 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st data    : Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "1st filename: C:\\Users\\Dell\\scikit_learn_data\\20news_home\\20news-bydate-train\\comp.graphics\\38816\n",
      "\n",
      "1st target  : 1\n"
     ]
    }
   ],
   "source": [
    "print('1st data    :',data_train['data'][0])\n",
    "print('\\n1st filename:',data_train['filenames'][0])\n",
    "print('\\n1st target  :',data_train['target'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:\n",
    "\n",
    "3. The 1st data point is from the 'comp.graphics' category (1st category in the 'target_names' feature). It looks like a query, a forum post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create a bag-of-words model.\n",
    "\n",
    "Let's train a model using a simple count vectorizer.\n",
    "\n",
    "1) Initialize a standard CountVectorizer and fit the training data.\n",
    "- How big is the feature dictionary?\n",
    "- Eliminate English stop words.\n",
    "- Is the dictionary smaller?\n",
    "- Transform the training data using the trained vectorizer.\n",
    "- Evaluate the performance of a logistic regression on the features extracted by the CountVectorizer.\n",
    "    - You will have to transform the `test_set`, too. Be careful to use the trained vectorizer without refitting it.\n",
    "\n",
    "**Bonus**\n",
    "- Try a couple of modifications:\n",
    "    - Restrict the `max_features`.\n",
    "    - Change the `max_df` and `min_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do it without functions (cos that's meant for the bonus at the end of the lab)... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data_train words first...\n",
    "\n",
    "# instantiate empty list to hold tokenized words, and those with stopwords removed/stemmed/joined\n",
    "train_words, train_words_nostop, train_words_nostop_stem, train_words_nostop_stem_join=[],[],[],[]\n",
    "\n",
    "# instantiate tokenizer, then tokenize the lower case of text\n",
    "tokenizer = RegexpTokenizer('\\w+')    #only extract words, not spaces/punctuations\n",
    "for i in range(len(data_train['data'])):\n",
    "    train_words.append(tokenizer.tokenize(data_train['data'][i].lower()))\n",
    "\n",
    "# remove stopwords, compile into a list. Or, dont do this here, instead let CountVectorizer do it later on\n",
    "# stops = set(stopwords.words('english')) #runs faster if its a set\n",
    "stops = []\n",
    "for i in range(len(train_words)):\n",
    "    train_words_nostop.append([w for w in train_words[i] if w not in stops])\n",
    "\n",
    "# instantiate snowballstemmer, then stem\n",
    "s_stemmer = SnowballStemmer('english')\n",
    "for i in range(len(train_words_nostop)):\n",
    "    train_words_nostop_stem.append([s_stemmer.stem(j) for j in train_words_nostop[i]])\n",
    "\n",
    "# Join the words back into one string separated by space, and return the result.\n",
    "for i in range(len(train_words_nostop_stem)):\n",
    "    train_words_nostop_stem_join.append(\" \".join(train_words_nostop_stem[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data_test words too...\n",
    "\n",
    "# instantiate empty list to hold tokenized words, and those with stopwords removed/stemmed/joined\n",
    "test_words, test_words_nostop, test_words_nostop_stem, test_words_nostop_stem_join=[],[],[],[]\n",
    "\n",
    "# instantiate tokenizer, then tokenize the lower case of text\n",
    "tokenizer = RegexpTokenizer('\\w+')    #only extract words, not spaces/punctuations\n",
    "for i in range(len(data_test['data'])):\n",
    "    test_words.append(tokenizer.tokenize(data_test['data'][i].lower()))\n",
    "\n",
    "# remove stopwords, compile into a list. Or, dont do this here, instead let CountVectorizer do it later on \n",
    "# stops = set(stopwords.words('english')) #runs faster if its a set\n",
    "stops = []\n",
    "for i in range(len(test_words)):\n",
    "    test_words_nostop.append([w for w in test_words[i] if w not in stops])\n",
    "\n",
    "# instantiate snowballstemmer, then stem\n",
    "s_stemmer = SnowballStemmer('english')\n",
    "for i in range(len(test_words_nostop)):\n",
    "    test_words_nostop_stem.append([s_stemmer.stem(j) for j in test_words_nostop[i]])\n",
    "\n",
    "# Join the words back into one string separated by space, and return the result.\n",
    "for i in range(len(test_words_nostop_stem)):\n",
    "    test_words_nostop_stem_join.append(\" \".join(test_words_nostop_stem[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate vectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "#                              stop_words='english', #define what are our stopwords (from the 'english' list)\n",
    "#                              max_df=0.7,           #ignores frequent words that appear in eg.>70% of datapoints, or say >6 datapoints\n",
    "#                              min_df=0.1,           #ignores infrequent words that appear in eg.<10% of datapoints, or say <6 datapoints\n",
    "#                              max_features=5000       #limits the number of features/words in the vectorizer\n",
    "                            )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit transform from train, then restrict test data to just those features\n",
    "train_data_features = vectorizer.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = vectorizer.transform(test_words_nostop_stem_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 19619)\n",
      "(1353, 19619)\n"
     ]
    }
   ],
   "source": [
    "# see no. of features for train and test data\n",
    "print(train_data_features.shape)\n",
    "print(test_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without removing stopwords, we have a huge number (19619) of features. Now, lets remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate vectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             stop_words='english', #define what are our stopwords (from the 'english' list)\n",
    "#                              max_df=0.7,           #ignores frequent words that appear in eg.>70% of datapoints, or say >6 datapoints\n",
    "#                              min_df=0.1,           #ignores infrequent words that appear in eg.<10% of datapoints, or say <6 datapoints\n",
    "#                              max_features=5000       #limits the number of features/words in the vectorizer\n",
    "                            )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit transform from train, then restrict test data to just those features\n",
    "train_data_features = vectorizer.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = vectorizer.transform(test_words_nostop_stem_join)  #transform only! not fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 19400)\n",
      "(1353, 19400)\n"
     ]
    }
   ],
   "source": [
    "# see no. of features for train and test data\n",
    "print(train_data_features.shape)\n",
    "print(test_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fewer features (19400) after removing stopwords (but removing stopwords in the CountVectorizer function, is less preferred than removing it earlier outside of the function, via nltk's library. This is because CountVectorizer uses a poorer set of stopwords from sklearn). Feature dictionary certainly decreased in size. \n",
    "\n",
    "Now let's pass them through a logreg model, to see how well the test data performs, through learning from the train data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set y's for train, test datasets \n",
    "y_train = data_train['target']\n",
    "y_test = data_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9768928220255654\n",
      "0.7361419068736141\n"
     ]
    }
   ],
   "source": [
    "# initiate logreg model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# train on train data, then score it\n",
    "logreg.fit(train_data_features, y_train)\n",
    "print(logreg.score(train_data_features, y_train))\n",
    "\n",
    "# now, score and evaluate model on test data\n",
    "print(logreg.score(test_data_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy was very high (98%) for training data, but reduced for test data (74%).\n",
    "\n",
    "Now lets fiddle with the max_df, min_df, max_features arguments in CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing 3 different types of vectorisers, each varying its maxdf/mindf/maxfeatures value. So 6 vectorizers\n",
    "\n",
    "vectorizer_maxdf7 = CountVectorizer(analyzer='word',\n",
    "                             stop_words='english',\n",
    "                             max_df=0.7,           #ignores frequent words that appear in eg.>70% of datapoints, or say >6 datapoints\n",
    "                            )\n",
    "\n",
    "vectorizer_maxdf2 = CountVectorizer(analyzer='word',\n",
    "                             stop_words='english',\n",
    "                             max_df=0.2,           #ignores frequent words that appear in eg.>20% of datapoints, or say >6 datapoints\n",
    "                            )\n",
    "\n",
    "vectorizer_mindf1 = CountVectorizer(analyzer='word',\n",
    "                             stop_words='english',\n",
    "                             min_df=0.1,           #ignores infrequent words that appear in eg.<10% of datapoints, or say <6 datapoints\n",
    "                            )\n",
    "\n",
    "vectorizer_mindf2 = CountVectorizer(analyzer='word',\n",
    "                             stop_words='english',\n",
    "                             min_df=0.2,           #ignores infrequent words that appear in eg.<20% of datapoints, or say <6 datapoints\n",
    "                            )\n",
    "\n",
    "vectorizer_maxfeatures9000 = CountVectorizer(analyzer='word',\n",
    "                             stop_words='english',\n",
    "                             max_features=9000       #limits the number of features/words in the vectorizer  \n",
    "                            )\n",
    "\n",
    "vectorizer_maxfeatures1000 = CountVectorizer(analyzer='word',\n",
    "                             stop_words='english',\n",
    "                             max_features=1000       #limits the number of features/words in the vectorizer  \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score with maxdf 0.7: 0.9768928220255654\n",
      "Test score with maxdf  0.7: 0.7361419068736141\n",
      "Train score with maxdf 0.2: 0.9768928220255654\n",
      "Test score with maxdf  0.2: 0.7442719881744272\n"
     ]
    }
   ],
   "source": [
    "# varying maxdf\n",
    "\n",
    "# maxdf = 0.7\n",
    "# fit transform with new vectorizer\n",
    "train_data_features = vectorizer_maxdf7.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = vectorizer_maxdf7.transform(test_words_nostop_stem_join)\n",
    "\n",
    "# fit logreg model using these reduced features, and score for train and test data \n",
    "logreg.fit(train_data_features, y_train)\n",
    "print('Train score with maxdf 0.7:',logreg.score(train_data_features, y_train))\n",
    "print('Test score with maxdf  0.7:',logreg.score(test_data_features, y_test))\n",
    "\n",
    "# maxdf = 0.2\n",
    "# fit transform with new vectorizer\n",
    "train_data_features = vectorizer_maxdf2.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = vectorizer_maxdf2.transform(test_words_nostop_stem_join)\n",
    "\n",
    "# fit logreg model using these reduced features, and score for train and test data \n",
    "logreg.fit(train_data_features, y_train)\n",
    "print('Train score with maxdf 0.2:',logreg.score(train_data_features, y_train))\n",
    "print('Test score with maxdf  0.2:',logreg.score(test_data_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a low maxdf (ignoring frequent words/features that appear in >20% of the entries/datapoints in the train dataset), improves the accuracy. This stringency probably differentiates entries with unique words better, hence making predictions more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score with mindf 0.1: 0.567354965585054\n",
      "Test score with mindf  0.1: 0.516629711751663\n",
      "Train score with mindf 0.2: 0.37659783677482794\n",
      "Test score with mindf  0.2: 0.3614190687361419\n"
     ]
    }
   ],
   "source": [
    "# varying mindf\n",
    "\n",
    "# mindf = 0.1\n",
    "# fit transform with new vectorizer\n",
    "train_data_features = vectorizer_mindf1.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = vectorizer_mindf1.transform(test_words_nostop_stem_join)\n",
    "\n",
    "# fit logreg model using these reduced features, and score for train and test data \n",
    "logreg.fit(train_data_features, y_train)\n",
    "print('Train score with mindf 0.1:',logreg.score(train_data_features, y_train))\n",
    "print('Test score with mindf  0.1:',logreg.score(test_data_features, y_test))\n",
    "\n",
    "# mindf = 0.2\n",
    "# fit transform with new vectorizer\n",
    "train_data_features = vectorizer_mindf2.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = vectorizer_mindf2.transform(test_words_nostop_stem_join)\n",
    "\n",
    "# fit logreg model using these reduced features, and score for train and test data \n",
    "logreg.fit(train_data_features, y_train)\n",
    "print('Train score with mindf 0.2:',logreg.score(train_data_features, y_train))\n",
    "print('Test score with mindf  0.2:',logreg.score(test_data_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a low mindf (ignoring infrequent words/features that appear only in <10% of the entries/datapoints in the train/test dataset), improves the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score with maxfeatures 9000: 0.9759095378564405\n",
      "Test score with maxfeatures  9000: 0.738359201773836\n",
      "Train score with maxfeatures 1000: 0.9488692232055064\n",
      "Test score with maxfeatures  1000: 0.7095343680709535\n"
     ]
    }
   ],
   "source": [
    "# varying maxfeatures\n",
    "\n",
    "# maxfeatures = 9000\n",
    "# fit transform with new vectorizer\n",
    "train_data_features = vectorizer_maxfeatures9000.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = vectorizer_maxfeatures9000.transform(test_words_nostop_stem_join)\n",
    "\n",
    "# fit logreg model using these reduced features, and score for train and test data \n",
    "logreg.fit(train_data_features, y_train)\n",
    "print('Train score with maxfeatures 9000:',logreg.score(train_data_features, y_train))\n",
    "print('Test score with maxfeatures  9000:',logreg.score(test_data_features, y_test))\n",
    "\n",
    "# maxfeatures = 1000\n",
    "# fit transform with new vectorizer\n",
    "train_data_features = vectorizer_maxfeatures1000.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = vectorizer_maxfeatures1000.transform(test_words_nostop_stem_join)\n",
    "\n",
    "# fit logreg model using these reduced features, and score for train and test data \n",
    "logreg.fit(train_data_features, y_train)\n",
    "print('Train score with maxfeatures 1000:',logreg.score(train_data_features, y_train))\n",
    "print('Test score with maxfeatures  1000:',logreg.score(test_data_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a low maxfeatures (limits the no. of words/features that each entry/datapoint would have to define itself), worsens the accuracy. This is quite likely due to the reduced ability to identify an entry correctly, due to fewer features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Test Out Hashing and TF-IDF.\n",
    "\n",
    "Let's see if hashing or TF-IDF improves our accuracy.\n",
    "\n",
    "1) Initialize a HashingVectorizer and repeat the test with no restriction on the number of features.\n",
    "- Does the score improve with respect to the CountVectorizer?\n",
    "- Print out the number of features for this model.\n",
    "- Initialize a TF-IDF vectorizer and repeat the analysis above.\n",
    "- Print out the number of features for this model.\n",
    "\n",
    "**Bonus**\n",
    "- Change the parameters of either (or both) models to improve your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features 1048576\n",
      "Train score: 0.8456243854473943\n",
      "Test score : 0.6836659275683666\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "# instantiate hvec\n",
    "hvec = HashingVectorizer()\n",
    "# fit transform from train, then restrict test data to just those features\n",
    "train_data_features = hvec.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = hvec.transform(test_words_nostop_stem_join)\n",
    "print('No. of features',train_data_features.shape[1])\n",
    "\n",
    "# train on train data, then score it\n",
    "logreg.fit(train_data_features, y_train)\n",
    "print('Train score:',logreg.score(train_data_features, y_train))\n",
    "\n",
    "# now, score and evaluate model on test data\n",
    "print('Test score :',logreg.score(test_data_features, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features 19619\n",
      "Train score: 0.9336283185840708\n",
      "Test score : 0.7420546932742055\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "# instantiate tfidf\n",
    "tvec = TfidfVectorizer()\n",
    "# fit transform from train, then restrict test data to just those features\n",
    "train_data_features = tvec.fit_transform(train_words_nostop_stem_join)\n",
    "test_data_features = tvec.transform(test_words_nostop_stem_join)\n",
    "print('No. of features',train_data_features.shape[1])\n",
    "\n",
    "# train on train data, then score it\n",
    "logreg.fit(train_data_features, y_train)\n",
    "print('Train score:',logreg.score(train_data_features, y_train))\n",
    "\n",
    "# now, score and evaluate model on test data\n",
    "print('Test score :',logreg.score(test_data_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HashingVectorizer produces significantly more features than TFIDF, but performs worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. [Bonus] Robust Text Preprocessing\n",
    "\n",
    "Your mission, should you choose to accept it, is to write a preprocessing function for all of your text.  This functions should\n",
    "\n",
    "- convert all text to lowercase,\n",
    "- remove punctuation,\n",
    "- stem or lemmatize each word of the text,\n",
    "- remove stopwords.\n",
    "\n",
    "The function should receive one string of text and return the processed text.\n",
    "\n",
    "Once you have built your function, use it to process your train and test data, then fit a Logistic Regression model to see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My Approach:\n",
    "1. tokenize (simultaneously lower-casing, removing punctuation)\n",
    "2. stem\n",
    "3. remove stopwords\n",
    "4. join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in c:\\users\\dell\\anaconda3\\lib\\site-packages (2019.11.1)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all', quiet=True)  #set quiet=True to surpress loooong printouts\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "!pip install regex\n",
    "import regex as re\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'i', 've', 'noticed', 'that', 'if', 'you', 'only', 'save', 'a', 'model', 'with', 'all', 'your', 'mapping', 'planes', 'positioned', 'carefully', 'to', 'a', '3ds', 'file', 'that', 'when', 'you', 'reload', 'it', 'after', 'restarting', '3ds', 'they', 'are', 'given', 'a', 'default', 'position', 'and', 'orientation', 'but', 'if', 'you', 'save', 'to', 'a', 'prj', 'file', 'their', 'positions', 'orientation', 'are', 'preserved', 'does', 'anyone', 'know', 'why', 'this', 'information', 'is', 'not', 'stored', 'in', 'the', '3ds', 'file', 'nothing', 'is', 'explicitly', 'said', 'in', 'the', 'manual', 'about', 'saving', 'texture', 'rules', 'in', 'the', 'prj', 'file', 'i', 'd', 'like', 'to', 'be', 'able', 'to', 'read', 'the', 'texture', 'rule', 'information', 'does', 'anyone', 'have', 'the', 'format', 'for', 'the', 'prj', 'file', 'is', 'the', 'cel', 'file', 'format', 'available', 'from', 'somewhere', 'rych']\n"
     ]
    }
   ],
   "source": [
    "# instantiate tokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+')    #only extract words, not spaces/punctuations\n",
    "\n",
    "# test if our tokenizer works\n",
    "words = tokenizer.tokenize(data_train['data'][0].lower()) #tokenize the lower case of the text\n",
    "print(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original no. of words: 109\n",
      "New no. of words     : 54\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords. This is the preferred approach to removing stopwords - doing it before, outside of \n",
    "# the vectorizer function.\n",
    "print('Original no. of words:',len(words))\n",
    "words = [w for w in words if w not in stopwords.words('english')]\n",
    "print('New no. of words     :',len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'notic',\n",
       " 'save',\n",
       " 'model',\n",
       " 'map',\n",
       " 'plane',\n",
       " 'posit',\n",
       " 'care',\n",
       " '3ds',\n",
       " 'file',\n",
       " 'reload',\n",
       " 'restart',\n",
       " '3ds',\n",
       " 'given',\n",
       " 'default',\n",
       " 'posit',\n",
       " 'orient',\n",
       " 'save',\n",
       " 'prj',\n",
       " 'file',\n",
       " 'posit',\n",
       " 'orient',\n",
       " 'preserv',\n",
       " 'anyon',\n",
       " 'know',\n",
       " 'inform',\n",
       " 'store',\n",
       " '3ds',\n",
       " 'file',\n",
       " 'noth',\n",
       " 'explicit',\n",
       " 'said',\n",
       " 'manual',\n",
       " 'save',\n",
       " 'textur',\n",
       " 'rule',\n",
       " 'prj',\n",
       " 'file',\n",
       " 'like',\n",
       " 'abl',\n",
       " 'read',\n",
       " 'textur',\n",
       " 'rule',\n",
       " 'inform',\n",
       " 'anyon',\n",
       " 'format',\n",
       " 'prj',\n",
       " 'file',\n",
       " 'cel',\n",
       " 'file',\n",
       " 'format',\n",
       " 'avail',\n",
       " 'somewher',\n",
       " 'rych']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate snowballstemmer\n",
    "s_stemmer = SnowballStemmer('english')\n",
    "\n",
    "# stem our words\n",
    "words = [s_stemmer.stem(i) for i in words]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pack all into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi i ve notic that if you onli save a model with all your map plane posit care to a 3ds file that when you reload it after restart 3ds they are given a default posit and orient but if you save to a prj file their posit orient are preserv doe anyon know whi this inform is not store in the 3ds file noth is explicit said in the manual about save textur rule in the prj file i d like to be abl to read the textur rule inform doe anyon have the format for the prj file is the cel file format avail from somewher rych'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_words(data):\n",
    "    words = []\n",
    "    \n",
    "    # instantiate tokenizer\n",
    "    tokenizer = RegexpTokenizer('\\w+')    #only extract words, not spaces/punctuations\n",
    "    \n",
    "    # instantiate snowballstemmer\n",
    "    s_stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    #tokenize the lower case of the text\n",
    "    words.append(tokenizer.tokenize(data.lower()))\n",
    "    \n",
    "    # remove stopwords \n",
    "    words_nostop = [w for w in words if w not in stopwords.words('english')]\n",
    "\n",
    "    # stemming\n",
    "    words_nostop_stem = [s_stemmer.stem(i) for i in words_nostop[0]]\n",
    "    \n",
    "    # Join the words back into one string separated by space, and return the result.\n",
    "    return(\" \".join(words_nostop_stem))\n",
    "    \n",
    "clean_words(data_train.data[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
