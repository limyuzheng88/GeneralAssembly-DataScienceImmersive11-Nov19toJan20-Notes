{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from intro to pandas, concatenation, eda exercises:\n",
    "\n",
    "TO SPEED UP PYTOHN PANDAS!!! 'pip install modin', which uses 'ray' which also utilizes muti cores\n",
    "\n",
    "check out GA students' profiles https://profiles.generalassemb.ly/profiles\n",
    "\n",
    "# notes:\n",
    "\n",
    "## resume\n",
    "-SQL is SUPER IMPORTANT for interviews! wtv DSI teaches, is NOT ENOUGH! Also don't call rows as rows, call \"observations\"\n",
    "\n",
    "-interview practice: https://dev.to/seattledataguy/data-science-study-guide-1g0n\n",
    "\n",
    "-Modern Resume is preferred (cos succint). Traditional Resume too wordy, HR got no time to read thru\n",
    "\n",
    "-should not include in resume, non professional stuff: hobbies, personal address (if u stay in Jurong and applying to Changi, employers will think u travel far, and be more tired, less motivated), religion, nationality (cos employer may hv preference for sgrean/european etc), age/dob (else employer think u're too young/old), personal photo, references (lest my previous boss get caleld up and creates bias against me). All in all, reduce bias against me\n",
    "\n",
    "-for previous jobs, job title more important than company name. Your degree more imp than your school. Cos its your branding, what you've achieved\n",
    "\n",
    "-for each job i held, state at least 5 outcomes. Be consistent for all past jobs i include, cos if 1 job has fewer points, employer will think less of it\n",
    "\n",
    "-under skills category, state ur previous jobs' experience ~60%. GA hasnt transformed us into Data Scientists in 12wks \n",
    "\n",
    "-cover letter: title im applying for, job id u're applying for, wat im good at, exp, edu, include my soft skills, my research of the company/dept im applying to. Hence why im a good fit to the company/dept\n",
    "\n",
    "-don't appear desperate by stating \"i can accept smth else if not given Data Analyst role\"\n",
    "\n",
    "-NEVER put ur resume online/linkedin, else anyone can download freely, and you can't finetune it to suit the company's needs\n",
    "\n",
    "-is ok to brand myself as a Data Analyst (even tho im not 1 yet), cos its what im applying to be. Put it as Data Analyst, so that ppl can find it on linkedin\n",
    "\n",
    "-careful with adding hyperlink to a webpage that doesn't belong to me, lest webpage changes. Instead, put a screenshot\n",
    "\n",
    "-important to include as many skillsets in linkedin as possible, so that companies will see that i match their required skills\n",
    "\n",
    "-on linkedin, after clicking the company, DON'T click blue button \"Apply\". Instead go to the original company website to apply, cos the job posting may not even be avail anymore on their company website (might be taken le). ALSO, the company clearly would prioritize applicants on their own web portal first, before searching up on linkedin\n",
    "\n",
    "-go to the linked in website of the company you wanna apply to, message the staff there holding similar roles to wat u wanna apply for, to understand their roles better\n",
    "\n",
    "-the more connections u hv on linkedin, the easier to reach out to strangers on linkedin cos of 2/3degree connections\n",
    "\n",
    "## general\n",
    "-use \"sci-hub.se\" to bypass paywall for scientific journals\n",
    "\n",
    "-use \"Epic privacy browser\" to bypass paywall. Its based off chrome. Comes with its own inbuilt vpn too\n",
    "\n",
    "-to google precisely for something, google it with open inverted commas.\n",
    "\n",
    "-dont import a library, if you dont need it. Else will sap computer memory\n",
    "\n",
    "-when u newly open jupyter, wanna run all code from top to bottom: Kernel->restart kernel (so as to wash away all memory in case previous variables still remain in use), then Cell->Run All\n",
    "\n",
    "-when we seed random results, always hv to seed just before each line of randomiser code. Simply having 1 seed line at the beginning of code, DOESN'T apply to all randomiser code lines \n",
    "\n",
    "-column wise operation is faster than row wise. hence use \"map with lambda\" is faster, not for loops\n",
    "\n",
    "-map works on columns, series. apply works on both columns and rows\n",
    "\n",
    "-to identify that the index column is the index, notice that it has an empty space above the title of the index, not aligned with the row of column names\n",
    "\n",
    "-to install packages, go anaconda prompt, type pip install <wtv this library is>, ENTER\n",
    "    \n",
    "-to create different environment, use Anaconda Navigator's gui, click \"Create\" bla bla. To work on a jupyter notebook from another environment, click that environment's right arrow, \"Open with Jupyter Notebook\". To pip install in that environment, also click that environment's right arrow, \"Open Terminal\" and pip install there\n",
    "\n",
    "-clustering/grouping looks across ALL features (not just 1 feature) and groups some tgt\n",
    "\n",
    "-to replace strings with smth across rows in 1 col of df, use 'where'\n",
    "\n",
    "data['win'] = np.where(data['HostName']==data['winner'],1,0)\n",
    "\n",
    "-default .score for linreg, lasso/cv, ridge/cv, is R2. Default .best_score_ for knn is Accuracy (=(true +ve plus true -ve)/total no. of data pts)\n",
    "\n",
    "-R2 (relative score) is better than RMSE (absolute score). RMSE of 0.1 DOESN'T mean its better than RMSE of 100, cos if a project has \n",
    "\n",
    "-Accuracy is not good, cos when there is unbalanced classes (99% class 1, 1% class 0), and all 99 got accurately classified as class 1, then although you will have 99% accuracy, but its just performing to baseline\n",
    "\n",
    "-ROC curve is Receiver Operator Characteristic (ROC) curve, AUC is Area Under Curve\n",
    "\n",
    "-how to tell we're overfitting? use this Diagnostic Test: see accuracy of prediction on training set, meaning you PREDICT AND SCORE on TRAIN SET (unlike the usual predict on test set). If prediction score of train set (~95%), is >> than predicted score of test set (~65%)\n",
    "\n",
    "-with all ML, always 1st seek to be highly explainable (else bosses wont even accept), then be highly accurate. Decision Trees good in this aspect, and are resistant to outliers \n",
    "\n",
    "-VERY IMPORTANT to know sql!!\n",
    "\n",
    "-to boost speed of python on laptop: on anaconda prompt, do 'pip install modin'. Modin utilises ALL cores, but pandas by default uses only 1 core (https://towardsdatascience.com/get-faster-pandas-with-modin-even-on-your-laptops-b527a2eeda74). when importing, type 'import modin.pandas as pd', but it may not have the full suite of features as normal pandas though. Also can look at swifter (https://towardsdatascience.com/add-this-single-word-to-make-your-pandas-apply-faster-90ee2fffe9e8). Do \"conda install -c conda-forge swifter\", then... \n",
    "\n",
    "\"import swifter\n",
    "\n",
    "pdf['e'] = pdf.swifter.apply(lambda x : func(x['a'],x['b']),axis=1)\"\n",
    "\n",
    "-when making a website, use a responsive html template, so that it conforms to the screen of phone/laptop flexibly\n",
    "\n",
    "-use herokuapp, to deploy on website\n",
    "\n",
    "-chatbots: use facebook's wit.ai (free?), google dialogflow (not free?), AWS Polly (pay per use), AWS Lex (pay per use)\n",
    "\n",
    "## intro to pandas\n",
    "-try not to run memory intensive work on jupyter on chrome, cos chrome is v mem intensive. can run from cmd not chrome,\n",
    "its less intensive\n",
    "\n",
    "-xls xlsx xml csv json txt html blob, are all file formats that we can use with pandas\n",
    "\n",
    "-python can show more rows, cols than excel can\n",
    "\n",
    "-python can load small chunks of data, instead of loading the whole 1 billion row file\n",
    "\n",
    "-dictionaries cannot be a column name\n",
    "\n",
    "-dont read out the whole dataframe cos v memory intensive, use head to just print out the top few rows\n",
    "\n",
    "-1 column in a dataframe, is a series or a list\n",
    "\n",
    "-dataframe can be split into multiple chunks for different cpu cores to work on, but series/lists cannot (worse off)\n",
    "\n",
    "-big data processing = distributed computing, where u split up processing and let multiple computers compute in parallel\n",
    "\n",
    "-loc for identifying (rows and) column by name, iloc is by integer position of row and column\n",
    "\n",
    "-see Sorting ex: do create a new column 'Time_2', so as to retain the raw data\n",
    "\n",
    "-see Now You!: when reading in files, use ../ and then 'tab' to let jupyter smartsearch through your files\n",
    "\n",
    "-in drop, if inplace is True, it returns nth. hence cannot do:\n",
    "cars.drop('kpg',axis=1,inplace=True).head()\n",
    "cos after performing the drop operation, there's nth produced for head()to act on. See documentation:\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
    "inplace : bool, default False\n",
    "If True, do operation inplace and return None.\n",
    "\n",
    "-MTurk, or Mechanical Turk, by amazon, allows you to pay cents to get thousands of ppl to help you do a task, could be labelling photos etc \n",
    "\n",
    "# concatenation\n",
    "-RDBMS = relational database mgmt system. can query its data via sql language\n",
    "\n",
    "-concatenate can happen EITHER hori or vert. but merge mostly happens hori\n",
    "\n",
    "-when joining/merging, we join based on a common key column \n",
    "\n",
    "-when we join/merge, the no. of rows/cols balloon! cos every row in df1 (eg. total 1000rows), \n",
    "joins w every matching row in df2 (eg. total 3000 matching rows with each row in df1), =3mil rows created!\n",
    "v expensive operation\n",
    "\n",
    "-\"pd.DataFrame\", NOTE that D and F are capital letters!!!\n",
    "\n",
    "-dictionaries can be used to create a dataframe. See section: \"Concatenation using Pandas\"\n",
    "\n",
    "-Primary Key: an index/column where all values in it, are unique (eg. userID)\n",
    "Composite Key: combined indexes/columns where all values in this combined index/col, are unique (eg. UserID concat\n",
    "with PurchaseID)\n",
    "\n",
    "-if you wanna merge and filter, more efficient to filter FIRST, then merge. Cos if u gonna throw away data eventually,\n",
    "then might as well filter first. Else merge will be computationally heavy\n",
    "\n",
    "# eda\n",
    "-within Jupyter notebook, to see documentation of a function, place your cursor in the boxplot argument bracket,\n",
    "and press shift+tab (Press four times repeatedly to bring up detailed documentation).\n",
    "\n",
    "-within j notebook, to see what functions \"blahblah\" can be called out from eg.scikit.metric.blahblah, just type \"tab\" after the last period\n",
    "\n",
    "-use help(wtv function) within Jupyter notebook to see documentation in J.notebook\n",
    "\n",
    "# 3.01 lab: sacramento housing\n",
    "-dummy variables: if wanna dummy zip codes, you'll get huuuge number of dummy categories. maybe categorise them by 1st 3digits, then fewer number of categories \n",
    "\n",
    "# 3.03 bias tradeoff\n",
    "-machine learning can be either real time (ML algo runs and produces output in realtime every time. Of course, hyperparameters change in realtime), or batch (ML first runs and outputs results, and remembers hyperparameters for eg. a week, and for that 1 week, will just respond based on input without ALWAYS running ML. After that 1 week, algo relearns with the week's new data and modifies its own hyperparameters, and remembers it for the coming week. ->batch by batch). batch is cheaper, faster, better ROI, cos no need powerful computers to run ML everytime on the fly\n",
    "\n",
    "# 3.04\n",
    "-train (70% of data), then test (eg. 20% of data), then a 2nd test would be Validation (eg. remaining 10% of test). Cos its unlikely that the model can fit well even on a 2nd test\n",
    "\n",
    "# feature selection\n",
    "-use techniques like recursive feature selection (rfe), stepwise, enter, forward/backward selection etc. see below\n",
    "\n",
    "-general steps to follow (google 'CRISP DM'):\n",
    "\n",
    "0. business problem\n",
    "1. get data\n",
    "2. explore + cleanse (anomalies)\n",
    "3. EDA (corr., heatmap, scatterplots, pairplots)\n",
    "4. Modelling\n",
    "    \n",
    "    a. feature selection (RFE which is the best, RFECV = RFE with cross validation which is even better, forward/backward selection, stepwise)\n",
    "    \n",
    "    b. instantiate model\n",
    "    \n",
    "    c. train test split, cross validate\n",
    "    \n",
    "    d. evaluation (highest R2, highest adj R2, highest accuracy, discard var with highest Variation Inflation Factor/highest P in OLS/stats.sm summary, if equal P then look at the beta/coefficient which is essentially the B0/B1/B2 in y=B0 + B1*x1 + B2*x2 +...)\n",
    "    \n",
    "-R2 is usually returned instead of adj R2, cos R2 is often used to derive other metrics. Its a useful value to return\n",
    "\n",
    "# 3.06 regularisation/regression\n",
    "-SSE = sum of sq errors = sigma [(y-y_hat)^2], is the J(theta)=the loss function\n",
    "\n",
    "-SST = sigma [(y-y_bar)^2], SSR = sigma [(y_hat-y_bar)^2]\n",
    "\n",
    "-discard variables that hv low coeff\n",
    "\n",
    "-Lasso regression is L1 regularization, Ridge regression is L2 regularization (ez to rmb: Lasso, Ridge->Left, Right, L1, L2)\n",
    "\n",
    "-J theta (where thetas are just betas) = RSS (beta0, beta1, beta2, beta3...) + alpha * sum (beta1^2, beta2^2, beta3^2...) \n",
    "\n",
    "-Ridge, Lasso, ElasticNet (combi of both Ridge n Lasso) are techniques used for Linear regression models. For non linear models, similar methods exist, but called different names\n",
    "\n",
    "-which to choose? If hv irrelevant features, use Lasso (cos it can zero out irrelevant features). if want best of both worlds, use ElasticNet. It has roe, which gives larger weight to Lasso and lower to Ridge (or the reverse) with a high roe\n",
    "\n",
    "-Ridge, Lasso can be v computationally expensive. Hence we only use it for late stage tuning, AFTER we have done train test split, cross val score, feature selection\n",
    "\n",
    "-we give a range of alphas, for algo to optimize itself\n",
    "\n",
    "-standardize/normalise first b4 Ridge/Lasso\n",
    "\n",
    "-no need scale/normalize target var, cos its a standalone col, not like the var df, which has many cols each with different ranges\n",
    "\n",
    "-Cheatsheet for Linear Reg (its a baseline before we move on to use more complex models):\n",
    "\n",
    "-ONLY transform test set (DONT fit!!!). But for train set, do fit AND transform\n",
    "\n",
    "-ONLY use cross val score on train data. DON'T use on test data, its totally not the point of doing cross val score\n",
    "\n",
    "-you WANT to see some difference in the different cross val scores. if u set cv=5, and all 5 are similar, its WRONG!\n",
    "\n",
    "-for train test split, for the X dataset, VERY IMPORTANT to keep in double brackets, for X train!\n",
    "\n",
    "-for train test split, VERY IMPORTANT to sort the rows randomly, lest eg. height column is arranged in descending order, and you manually take first 80 rows to train, last 20 to test, and that'll mean you're training on tall people and testing on short ones, hence results obviously will be bad. Good news is, train_test_split auto randomizes the selection for us\n",
    "\n",
    "-tricks for data cleaning:\n",
    "\n",
    "    a. if mode for a feature make up >80% in that feature (eg. >80% are 'Yes' in 'Got pool?' feature), then there is little variation in that feature, hence practically doesn't affect the target much. Hence should remove\n",
    "    b. for some categorical features, if NA, may not necesarily be NA, but may actually just mean it isn't present. eg. NA for 'Got pool?' feature may just mean 'no pool'\n",
    "    c. if u wanna remove outliers for datapoints which are >3 or 5 etc standard distribution, gotta make sure data is normally distributed first, before you can apply the concept of 'std distribution'\n",
    "    d. for linreg/ridge/lasso, assumption is that linear model works, and hence residuals HAVE NORMAL DISTRIBUTION. Must check that it is so!\n",
    "    e. if datapoints are not linear/normally distributed, may 'natural log or lon' them, to make them so\n",
    "    f. can combine some features tgt (eg. yr built & yr sold, can be combined into age)\n",
    "    g. cull unecessary features first, before polyfit. else with nCr combinations, HUGE number of (useless) features\n",
    "    h. gotta FIRST cull numerical features which have correlation <0.3 with target, and only AFTER that, then we VIF/RFE (then ridge/lasso etc)\n",
    "    i. MUST say what performance metric you use\n",
    "    j. MUST show the baseline score, and final model's score. Else they see only the final score, without knowledge of how poor the baseline originally was, and they criticise you\n",
    "    k. MUST mention limitations/challenges of your model, to highlight to bosses that 'dont expect this to be perfect on future models'\n",
    "    l. at end of ppt, MUST answer to management the qns, \"so what next?\"\n",
    "\n",
    "-Approach for linreg, VIF/RFE, ridge/lasso/elasticnet + regularisation:\n",
    "\n",
    "1. start w basic LinReg. must: R2<0.75 or 0.8 (if too high, is overfit. Likewise for LogReg/KNN, >0.75-0.8 also is overfit!), coeff of features are not different by 5-10x (if disparity too large, then some var are having an excessively large weightage over others, and these var may actually not make any biz sense to be affectign the target that much), corr of xi to y can be high (even as high as >0.75), corr. of features are not too high like >0.75 (seek 0.4<corr.<0.75. if too high, =features are not independant enough, got multi collinearity problems). \n",
    "\n",
    "(Note that if xi and xj are too highly correlated >0.75, not good! cos when xi rise, xj ALSO rise, then you'll get double whammy impact when only xi rises)\n",
    "\n",
    "2. if 1. fails in any category, use Variable Inflation Factor (VIF). must: get all var with VIF <5 (if any too high, then they should be dropped. If any are equally high, then they are not independant enough, but cant just drop), coeff of features are not different by 5-10x. If not VIF, can also use RFE. Then train test split, then get the baseline Cross_val_score on train data, then score on test (but if poor R2 again here, re-tune alpha, or try another model)\n",
    "\n",
    "2.5 if not VIF, a simpler way is Variance Threshold (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html). Can set threshold as 0.1\n",
    "\n",
    "3. thereafter, then move on to Ridge/Lasso/ElasticNet to tune the alpha/hyper parameters better. must: get optimal alpha, then chuck that alpha back into the Ridge/Lasso/ElasticNet model to get the associated coeffs. From the associated coeffs, predict the y_hat and the prediction eqn line. \n",
    "\n",
    "-almost always, its good to stratify for train test split\n",
    "\n",
    "# 3.07 Model Workflow (sort of still on linear regression)\n",
    "-if pvalue of F test is <0.05, then u must reject your H0 and accept your model with your chosen variables. \n",
    "\n",
    "-even if pvalue of F test is <0.05, but if 1 of the variables has pvalue>0.05, then u must ditch that variable too, and re-run the calc\n",
    "\n",
    "# 4.01 Intro to classification, Logistic Regression\n",
    "-wont use linear regression for classification problems\n",
    "\n",
    "-use classification, when features are discrete/categorical\n",
    "\n",
    "-classification methods include: Artificial Neural Networks, Logistic Regression, k nearest neighbours, Decision Trees/Random Forests, Naive Bayes\n",
    "\n",
    "-to choose a model, look at (in order of priority): 1. speed, 2. explainability (cos if takes long to run, then project would be long over), 3. Performance (cos if not explainable, biz users wont be convinced)\n",
    "\n",
    "-logreg can be binomial (default), or even multinomial\n",
    "\n",
    "-logistic regression uses sigmoid function (y range is 0 to 1. drawback of the Sigmoid activation function is that it can cause the neural network to get stuck at training time if strong negative input is provided). tanh function (y range is -1 to 1.  advantage of this function is that strong negative inputs will be mapped to negative output and only zero-valued inputs are mapped to near-zero outputs.,So less likely to get stuck during training.) also works well. see https://towardsdatascience.com/introduction-to-artificial-neural-networks-ann-1aea15775ef9\n",
    "\n",
    "# 4.02 KNN\n",
    "-problematic when: 1. dataset is huuuuuge, 2. when dataset is already bias to start with\n",
    "\n",
    "# 4.03 classification metric\n",
    "-type1 error: false +ve, type2 error: false -ve. Your algo has to favour 1 outcome over another, cant value both. In the case of fraud detection, false +ve (falsely identifying genuine customer as fraud) will lose his loyalty (and maybe lose $40 from him) and he might perhaps naysay and hurt your biz. But false -ve (falsely identifying fraud customer as genuine) will lose HUGE money (maybe $40k), hence false -ve in this case is more painful, hence we gotta value it more than false +ve. Bottomline: Gotta choose 1 to value!\n",
    "\n",
    "-accuracy, error, sensitivity, specificity, precision, recall (https://classeval.wordpress.com/introduction/basic-evaluation-measures/)\n",
    "\n",
    "-Accuracy is likely the best metric to use for MNIST classification. Improperly classifying a number is equally bad, no matter what number you incorrectly predict. For example, misclassifying a 4 as a 3 or 5 or 9 is equally bad.\n",
    "Many of our other classification metrics (like sensitivity and specificity) don't easily generalize to classification with more than two classes.\n",
    "\n",
    "-precision is arguably most important (aim for high), = TP/(TP+FP). If =0.2, then for every 5 recommendations, 1 will be correct. Sensitivity is also v impt (aim for high), =TP/(TP+FN). This is important when identifying FN is critical\n",
    "\n",
    "-if too many class 1 data (99%), too few class 0 data (1%), should bootstrap (like in lab 4.02) to increase the number of class 0 data such that its ~50-50%. Else unbalanced classes would: a) make the logreg model learn wrongly, and mistakenly always predict 'class 1', b) have an excessively low sensitivity/specificity which is bad. That said, bootstrapping can also be bad cos when you created multiple replicas, you may amplify the bias\n",
    "\n",
    "-inbalanced class, can also do SMOTE (to increase data pts, by creating synthetic data pts. DON'T say \"fake\" data pts!!! Kena judge by management), or TOMEK-Links (to reduce data pts). SMOTE creates synthetic data pts, by creating new ones between existing ones (kinda like an interpolated pt), and you can do this indefinitely. Even better is SMOTE-TOMEK, which does either SMOTE or TOMEK automatically! Its like Elastic Net (which allows you to use either Ridge or Lasso or an inbetween). There's also SMOTEEN\n",
    "\n",
    "# 4.05 optimization gridsearch n hyperparameter tuning\n",
    "-gridsearch can either bruteforce search thru ALL possibilities, or randomly sample possibilities\n",
    "\n",
    "-RidgeCV, LassoCV both hv cross val incorporated within. eg. 3folds, so find optimal alpha from 1st 2 folds and test on 3rd fold, then repeat for folds 1&3 test on 2, then repeat for folds 2&3 test on 1.\n",
    "\n",
    "-when running gridsearchCV, by default, should use ALL cores to run the search. To do so, set  n_jobs=-1\n",
    "\n",
    "# 4.06 APIs and consumption\n",
    "-Use API (a function) to access program/results/uncleaned data on Database (DB), stored in SQL format (Paid/Proprietary: offered by Oracle, Microsoft (MSFT), MS-SQL. Free/Open source (eg. MySQL, PostGre SQL): either completely free, or the service provider provides consultancy service only. But for free/open source, you are responsible for DB maintenance yourself). DB has the data which the tech team would work on. Consumers (eg. marketing team) dont consume that, instead they consume static data (eg. dashboards, CSV files) on data warehouse  \n",
    "\n",
    "-excel can only store 1.4m rows. csv can store more, billions of bytes. but they're hard to join/merge. SQL makes it ez to join/merge, v fast too. Excel also runs on windows/mac, which is v resource heavy, hence use a unix machine to run the SQL, which makes it faster.\n",
    "\n",
    "-what do we ues DB for (which excel isnt meant for)? Create, Read, Update, Delete (CRUD). SQL is optimized for speed, excel is optimised for functionality hence slow\n",
    "\n",
    "-json files are v light, lighter than csv\n",
    "\n",
    "-graph API works faster than table API. eg. for fb, if you wanna see whether person A is remotely connected to person Z, if in a table, you'd hv to find friends of person A (say, B, C), then find friends of B (say P, Q) and C (say X, Y), then finally after searching thru PGXY, finally find person Z as a friend of person Y. This is slow. Graph API is waaaay faster\n",
    "\n",
    "-web scraping, every website has a robots.txt page, can refer to it: \"google.org/robots.txt\"\n",
    "\n",
    "-\"lazy loading\": loads as u scroll further down the page\n",
    "\n",
    "-to spoof a website into thinking that we're not accessing from a chrome/android phone etc, we can: within requests.get(headers = 'user_agent'), set the user_agent as any user agent (can find list of typical UAs for android/iphones, chrome etc from https://deviceatlas.com/blog/list-of-user-agent-strings, or literally ANY UA id). #user agent: can set as literally ANY id, eg. dsakjlfhkjbalwb, not necessarily Pony Inc 1.0. If you get blocked (cos you keep scraping from your own machine, even if using different UAs. cos website sees its always the same IP address scraping from them). If so, can try access other ppl's computers and scrape via theirs\n",
    "\n",
    "# 5.01 html, css\n",
    "-always there will be at least the head (only available from h1 largest font, to h6 smallest font), and the body\n",
    "\n",
    "-<ul   /ul> Unordered list: Used to create a list of related items, in no particular order. <ol   /ol> Ordered list: Used to create a list of related items, in a specific order.\n",
    "\n",
    "-<a href=\"url\"   /a> anchor tag, which stores hyperlinks to be presented on webpage\n",
    "    \n",
    "-'class' is like functions in python code. u create a class/function in the css notebook, then can call it to use it\n",
    "\n",
    "-to add multiple styles inline, use the ';'. eg. <p style=\"color: red;font-size:10%\">This is a paragraph.</p>. But honestly, its better to write the styles as classes in the css notebook, separate from the html notebook \n",
    "\n",
    "-steps to follow in general:\n",
    "\n",
    "1. define url\n",
    "2. response = request.get url to get all content from url\n",
    "3. html = response.text() to extract all texts\n",
    "4. soup = BeautifulSoup(html, 'lxml')\n",
    "5. search/find/find_all which are 'p', 'div', class_='todo' etc\n",
    "6. Iterate: process/slicing/dicing\n",
    "\n",
    "-check out dbpedia.com, it is the underlying db of wikipedia, all info shown in graphical way\n",
    "\n",
    "-good to use xpath, to extract EXACTLY what info we wanna scrape\n",
    "\n",
    "-outputs of your model can be in the form of APIs, CSVs (put into dashboards for C-suites to use. CSVs also used by marketing/sales team, and Data team too), SQLs. APIs & SQLs used by product/tech team\n",
    "\n",
    "# 5.04 intro to AWS\n",
    "-when choosing AMI, DONT use windows'. Use unix's Ubuntu\n",
    "\n",
    "-steps in AWS\n",
    "1. pick EC2\n",
    "2. search for ubuntu, pick 1 of those AMIs (eg. Ubuntu Server 16.04 LTS (HVM)). choose the free tiered one (eg. t2 micro)\n",
    "3. configure instance details: network/subnet/autoassign public IP is for security to prevent strangers from coming in\n",
    "4. add storage: volumetype 'iops' is for fast processing. \n",
    "5. add tags: add references/'hashtags'\n",
    "6. configure security group: you define here what type of requests can enter. Type 'SSH' etc means you're only accepting certain types of requests, eg. secured etc. Source '0.0.0.0./0' means ANY IP address=ANYONE can access your environment, is dangerous. \n",
    "7. Launch instance.\n",
    "7.1. (actually could have just skipped from step 2 to 8. Leave 3-7 as default)\n",
    "8. open PuttyGen (gotta download first, for windows). Load/Import the keypair you created on AWS (see here: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair). Save this .ppk key (its a pvt key. could save as same name as .pem file)\n",
    "9. see your running instance, look under Description tab. copy Public DNS (IPv4 or 6 wtv). Open Putty (not puttygen), under HostName, put as \"ec2-user@\" (if we chose the default AMI. Which is more user-friendly than ubuntu) or \"ubuntu@\" (if we're using an ubuntu AMI), then paste the public dns behind it, port can leave as 22. Under Category on left edge, expand SSH, click (not expand) Auth, browse and load the ppk file here. Leave checkboxes as they are, click open.\n",
    "10. If you chose ubuntus AMI... On the ubuntu \"cmd prompt\"-ish terminal (which came from opening putty), a. install packages to install into the AMI server, else b. create script to install your desired packages, else c. install Amazon's package (this one ain't free!). Every time rmb to close an instance (right click the instance->Instant State->Terminate) after you're done. And every time you wanna start an instant again, GOTTA re-install anaconda all over again, on the (likely new) AMI server \n",
    "\n",
    "9a. alternatively, run on gitbash (must be in the same directory as where your keypair is at) this line: ssh -i \"kp1.pem\" ec2-user@ec2-13-229-224-114.ap-southeast-1.compute.amazonaws.com. This could be different each time you run an instance, but can be found by: EC2 Running Instance->\n",
    "\n",
    "11. to install jupyter notebook on AWS, follw this (https://medium.com/@alexjsanchez/python-3-notebooks-on-aws-ec2-in-15-mostly-easy-steps-2ec5e662c6c6). If cannot unpack anaconda3, refer to this (https://github.com/conda/conda/issues/9345#issuecomment-560174633), which says \n",
    "\n",
    "\"This was also a problem for the latest anaconda3 on the free tier Amazon EC2 ubuntu instance:\n",
    "\n",
    "wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh\n",
    "bash Anaconda3-2019.10-Linux-x86_64.sh\n",
    "\n",
    "\n",
    "Unpacking payload ...\n",
    "  0%|                                             | 0/291 [00:00<?, ?it/s]\n",
    "Using the last release solved the issue:\n",
    "\n",
    "wget https://repo.anaconda.com/archive/Anaconda3-2019.07-Linux-x86_64.sh\n",
    "bash Anaconda3-2019.07-Linux-x86_64.sh\"\n",
    "\n",
    "(dont type code line by line on ubuntu/putty terminal. Do such development work on your development laptop, save the final script/.py (NOT .ipynb!) file, then finally run it on the cloud server.)\n",
    "\n",
    "-go google OLTP (for databases. DBs are for transactions, to store raw data), OLAP (for data warehouses. DWHs are for analytics, to store processed data which were processed from raw data. Store them prepackaged here, so that when others wanna access them, can just access from here). Amazon provides DWH services in their Redshift product\n",
    "\n",
    "-for free server, use Digital Ocean/Google Cloud\n",
    "\n",
    "# 5.05 NLP\n",
    "-difficulties lie in slang, tone, acronyms, context, nuance, double meanings\n",
    "\n",
    "-used for sentiment analysis, speech recognition\n",
    "\n",
    "-check out Name Entity Recognition (NER)\n",
    "\n",
    "# 5.03 intro to APIs and flask\n",
    "-how to set up flask and python env: https://www.twilio.com/docs/usage/tutorials/how-to-set-up-your-python-and-flask-development-environment\n",
    "\n",
    "-why have virtual environment? a. experimental sandbox, to test out programmes (without risking harming main machine) before rolling them out. b. can hv multiple versions of a software. eg. for a few clients, they use v1. for others they use v2, v3 \n",
    "\n",
    "-to create virtualenvs' flaskservice, follow GA notes (without the ~/ cos thats for macs. Also note that cd is to change dir, dir is to see all folders, cd ../ is to go up 1 level), dont use . bin/activate, instead do 'cd Scripts' (to go into Scripts not bin folder, cos bin is for macs), then 'activate'\n",
    "\n",
    "-save the py file (eg. service.py in the GA example), in the flaskservice folder\n",
    "\n",
    "-when doing 'export FLASK_APP=service.py\n",
    "export FLASK_DEBUG=1\n",
    "flask run', replace export with 'set' for windows \n",
    "\n",
    "-2-flask_virtualenv_setup.ipynb & 3-intro to flask: write the code in Atom/Sublime, NOT jupyter notebook!\n",
    "\n",
    "-note that all inputs entered by user on browser searchbox, will be a str, hence need to convert to float if required\n",
    "\n",
    "-dont put the model training code (eg. logreg.fit(...)) within the app.route function, else web loading will take v long, especially if training a large amt of data. Instead, put the model training code outside of the function \n",
    "\n",
    "-important to serialize then pickle using 'from sklearn.externals import joblib' then 'joblib.dump(eg logreg model, 'bla model name.pkl')' \n",
    "\n",
    "-for storage of pkl etc files for server to call them, check out Amazon S3, Filezilla (for FTP), WinSCP\n",
    "\n",
    "# 5.05 nlp1\n",
    "-inflections are variations of the same word. eg. ran/run/running\n",
    "\n",
    "-to avoid overfitting an nlp's logreg model, its good to reduce the number of features. Also, important to set a limit for number of features, else crazy many features (each word that's a non-stopword (stopwords are eg. a, the, an) in training set, is a feature)\n",
    "\n",
    "-learn about NLP from christopher manning \n",
    "\n",
    "-to do nlp, follows these steps:\n",
    "\n",
    "    1. tokenize, lowercase\n",
    "    2. remove stopwords (eg. the, a, i). NLTK's stopwords (this is preferred!), are more comprehensive than CountVectorizer's stopwords list. Hence use this, to remove stopwords in pre-proc stage! dont wait till CountVectorizer stage.\n",
    "    2.5 consider removing accentuated alphabets like french e, and change \"y'all\" to \"you all\"\n",
    "    3. lemmatize/stem (snowballstemmer=Porter2 is the best so far i think)\n",
    "    4. join words back into 1 string separated by space. Hence you'll have many rows/a list of summarized strings\n",
    "    5. vectorize.fit_transform train data, then vectorize transform the test data. (vectorize = 'bag of words' method)\n",
    "    6. train test split (could happen earlier), logreg\n",
    "    7. to try out multiple models (countvectorizer/logreg with different arguments), can use Pipeline, tgt with GridSearchCV\n",
    "    \n",
    "-vectorization/bag-of-words loses context of/relationship between words. eg. 'not happy' becomes 'not' and 'happy', and their combined meaning becomes lost\n",
    "\n",
    "-too many features/words (the columns. relative to no. of rows), causes overfit. Just like linreg (with too many features like x1, x2...x100000)\n",
    "\n",
    "# 5.06 nlp2\n",
    "-after CountVectorizer, get_feature_names is v useful to see what words those features/columns represent \n",
    "\n",
    "-max_df: if a word is found in eg. >99% of the docs, then might as well ignore it. Hence set max_df=0.99 \n",
    "\n",
    "-Term Freq-Inverse Document Freq (TF-IDF) vectorizer is waaay better than plain simple vectorization/bag of words. In a vectorized matrix of words/features found in every row/doc, TF (=freq of that term Ã· no. of all terms in that row or doc. Hence, the larger/more common the better) is row-wise, IDF (=log (no. of all docs Ã· no. of docs with that term. Hence, the smaller/rarer the better)) is col-wise\n",
    "\n",
    "# 5.07 naive bayes\n",
    "-naive bayes is baseline model, to comapre otehr methods against\n",
    "\n",
    "-bernoulli NB (for binary categories), multinomial NB (for multiple categories), gaussian NB (for continuous variables)\n",
    "\n",
    "-P(spamword|spampost) = P(spamword intersection spampost)/P(spampost). eg. if 100 posts (10 spamposts of which 2 spamposts contain spamwords, 90 not), hence P(spamword|spampost) = (2/100)/(10/100) = 2/10\n",
    "\n",
    "-todense() converts to a matrix (strictly 2dimensional), but toarray() converts to an ndarray (n-dimensional)\n",
    "\n",
    "-if test scores > training scores, means the features/patterns were stronger in test set. Not a bad thing! But ofc, for test set, it can be higher/lower sometimes\n",
    "\n",
    "-chi square NECESSARY! cos we're dealing with categorical features here, highly correlated features (eg. 'Berkshire' and 'Hathaway' always come together) will affect your results \n",
    "\n",
    "# 5.08 regex\n",
    "-https://regex101.com/ good website to understand/play with regex\n",
    "\n",
    "-[Tt]hat: to find 'That' and 'that' in the text, -[Tt]h[iu]s: to find 'T/this' and 'T/thus' in the text, -[Tt]h[^i][st] or [Tt]h[au][st]: to find 'T/that' and 'T/thus'\n",
    "\n",
    "# Capstone\n",
    "-figure out yourself: problem statement, get data, model\n",
    "\n",
    "-data: https://archive.ics.uci.edu/ml/index.php, https://flowingdata.com/2009/10/01/30-resources-to-find-the-data-you-need/, https://www.kaggle.com/datasets?sort=votes, https://www.kaggle.com/competitions?sortBy=latestDeadline&group=general&page=1&pageSize=20, https://www.researchgate.net/post/Does_anybody_have_real_predictive_maintenance_data_sets, https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/, https://github.com/awesomedata/awesome-public-datasets, https://registry.opendata.aws/, https://www.drivendata.org/competitions/, https://tianchi.aliyun.com/competition/gameList/activeList, https://tianchi.aliyun.com/dataset?spm=5176.12281978.0.0.71b645b94IkiW2, https://www.machinehack.com/course-cat/modeling/\n",
    "\n",
    "-whats a project that'll be commonly applicable to many industries? B2C (business to consumer) industries that have: customers, products, suppliers. \n",
    "    - customer: customer segmentation/engagement/acquisition/churn\n",
    "    - product: product usage/types, team internal processes\n",
    "    - supply: supply/demand, logistics/movement of goods\n",
    "\n",
    "-google Bain/KPMG/BCG/Mckinsey/data science industry for ideas\n",
    "\n",
    "-how to work on LARGE files eg. <300MB? export as .py, run on cmd prompt (to troublshoot, best store its log or print out results) will be fastest. Can check your ram GB by typing 'dxdiag' into cmd prompt to open DirectX Diagnostic Tool, and if ram only 8gb, clearly you can't work on 10gb data, gotta look to AWS/cloud\n",
    "\n",
    "-alibaba dataset ideas: \n",
    "    - who are likely to be repeat customers (dont waste money on disloyal ones. spend more on loyal ones. or even convert disloyal ones)? \n",
    "    - who can be categorised into the same group based on purchase history, who might buy same items, hence can cross-sell (recommendation engine based on similar product tags, or similar tastes as other users. Not just users to users, but also match a demographic group who's the 'preference doppelganger' to another user eg. same age n gender)?\n",
    "    - Include fake ratings so that i can further demonstrate user-based collaborative based filtering and de-meaning  (like for movie recommendations)? \n",
    "    - Parse amazon reviews and append to mine? \n",
    "    - build web app to simulate real time product recommendation plus determine to give me voucher if im a loyal/disloyal customer? to store learnt model in database, for software engineer to extract (note that even for netflix, training doesnt happen real time)\n",
    "    - can incorporate sql?\n",
    "    - can include TFIDF on searchbar of the mock amazon website?\n",
    "\n",
    "# 5.09 Object orientated programming\n",
    "-OOP=splitting different tasks into functions, clump them tgt into 1 object/class, Include the data thats unique to the object itself. Data AND processing tgt in a class\n",
    "\n",
    "-Function based programming (FP) = same data can be shared across different objects, but functions are unique to each object. Data separate from processing, not in a class tgt\n",
    "\n",
    "-https://www.educba.com/functional-programming-vs-oop/\n",
    "\n",
    "-compilers translate human language into computer readable language\n",
    "\n",
    "-IMPORTANT for ALL functions, to have 'self' as an argument!!\n",
    "\n",
    "-in a Class, can hv multiple functions with the same name (called 'function overloading'), BUT MUST HAVE different arguments. If functions have similar arguments (eg. 2 integers), then for each function, you gotta input the arguments (which gotta have different names for both functions) explicitly (eg.var1=4 and not simply 4) \n",
    "\n",
    "-put 2 underscores infront of function name, if u wanna make it a private method. No one can call it (to protect it from use. eg. bank customer not supposed to utilise the function to earn interest as n when), not even by calling the function with its 2 underscores in front\n",
    "\n",
    "# 6.01 decision trees\n",
    "-summary of differences:\n",
    "\n",
    "datasets, features, split technique:\n",
    "1. DT            - 1, all, optimal\n",
    "\n",
    "2. Bagged        - many (cos bootstrap), all, optimal\n",
    "\n",
    "3. Random forest - many (cos bootstrap), random subset, optimal\n",
    "\n",
    "4. Extra trees   - 1, random subset, optimal\n",
    "\n",
    "-good to plot out the rule-set (=the tree)\n",
    "\n",
    "-https://victorzhou.com/blog/gini-impurity/\n",
    "\n",
    "-decision trees are always supervised learning (=targets are labelled)\n",
    "\n",
    "-MUST get a pure outcome (only 1 outcome eg. Mexican food, not impure eg. 75% chance Mexican, 25% Jap food) at the bottomost of the decision tree!\n",
    "\n",
    "-either use DecisionTreeRegressor, or DecisionTreeClassifier function\n",
    "\n",
    "-scores using gini impurity score n mse\n",
    "\n",
    "-decision trees being greedy will tend to overfit, learns even all the noise\n",
    "\n",
    "-decision trees need to keep re-training all over again, cos data may keep changing\n",
    "\n",
    "# 6.02 ensemble methods, and bagging\n",
    "-we use ensemble methods (averaging the results of multiple models: linreg/ridge/lasso, logreg, KNN, naive bayes(binomialNB/multinomialNB/gaussianNB), countvectorizer/tfidfvectorizer, decisiontree) cos we can then tap on strengths of each model (eg. DecisionTree overfits, so incorporate logreg too. but logreg may be susceptible to outliers, but DecisionTree isnt, so tap on decisiontree). \n",
    "\n",
    "As notes says: \"The statistical benefit to ensemble methods: By building one model, our predictions are almost certainly going to be wrong. Predictions from one model might overestimate housing prices; predictions from another model might underestimate housing prices. By \"averaging\" predictions from multiple models, we'll see that we can often cancel our errors out and get closer to the true function  ð‘“ .\n",
    "\n",
    "also, It might be impossible to develop one model that globally optimizes our objective function. (Remember that CART reach locally-optimal solutions that aren't guaranteed to be the globally-optimal solution.) In these cases, it may be impossible for one CART to arrive at the true function  ð‘“ . However, generating many different models and averaging their predictions may allow us to get results that are closer to the global optimum than any individual model.\n",
    "\n",
    "also, The representational benefit to ensemble methods: Even if we had all the data and all the computer power in the world, it might be impossible for one model to exactly equal  ð‘“ . For example, a linear regression model can never model a relationship where a one-unit change in  ð‘‹  is associated with some different change in  ð‘Œ  based on the value of  ð‘‹ . All models have some shortcomings. (See the no free lunch theorems.) While individual models have shortcomings, by creating multiple models and aggregating their predictions, we can actually create predictions that represent something that one model cannot ever represent.\n",
    "\n",
    "We can summarize this as the wisdom of the crowd.\"\n",
    "\n",
    "-When we ensemble, and get the mean scores of all models, will Notice that some probabilities are non 0 or 1. These are the 'contentious' ones, which indicate that the models are not well trained for these data, means maybe got some features which are not well trained in. Focus on these to improve model, or ignore these models.\n",
    "\n",
    "-bagging, or 'growing more trees': This 'multiple bootstrapped dataset, multiple trees' method, will have Higher score than the earliest 'single dataset, single tree' method, and also higher than the 'single dataset, multiple trees' method.\n",
    "\n",
    "-use BaggingClassifier to auto create 'multiple bootstrapped dataset, multiple trees'. Yes, bootstrapping is True by default, and 10 trees created by default. Just use BaggingClassifier to shortcut the whole process of creating \n",
    "\n",
    "# 6.02 random forest\n",
    "-random forest reduces overfit/high variance, hence removing high correlation features, by...\n",
    "\n",
    "-in random forest, at each node, we dont look at ALL features. we only look at a subset of wtv remaining reatures at that node/layer. Eventually, random forest trees will have fewer features/levels, but bagged (without limitation on depth) will have full number of features/levels\n",
    "\n",
    "-we dont limit depth of tree (but in bagged, we do, to prevent overfit)\n",
    "\n",
    "-extra trees classifier: also many trees (like random forest), but datasets are as-is and not bootstrapped (unlike random forest which has many slightly different datasets due to bootstrap), and the split (eg. to maximise the drop in gini impurity for random forest) is random (unlike random forest which chooses the best split). For the 3rd factor, hence, ET may have deep depth, RF would be computationally expensive\n",
    "\n",
    "-rf has significantly smaller depth than decision trees cos at every layer, a random subset of the sqrt or 1/3 of no. of features are split optimally at this level (vs extra trees: dont boot strap, similarly random subset, but random split)\n",
    "\n",
    "-VERY USEFUL: to first see whats a sensible no. of layers of depth, by:\n",
    "    - for estimator in rf.estimators_:\n",
    "        - print(estimator.tree_.max_depth)\n",
    "        \n",
    "# 6.03 boosting\n",
    "-improvement from bagging/random forest/extra tree clasifier (here, all trees are built simultaneously), by training 1 model (1st model will be a terribly weak learner) first then applying the lessons learnt there to the next model (also still quite a weak learner), and so on, iteratively. To iteratively improve the score for models (early weak learners initially score similarly to baseline eg. mean score of 50%. But iterative runs in the training improves it to 55%). The model improves iteratively to become a ~strong learner. https://codesachin.wordpress.com/2016/03/06/a-small-introduction-to-boosting/\n",
    "\n",
    "-https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5, http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf, https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5\n",
    "\n",
    "-retraining is done on the misclassified samples\n",
    "    \n",
    "-But time consuming, cos can't run computations in parallel. Also, high risk of overfitting cos with each iteration, the score improves, and we risk overfitting to training data\n",
    "\n",
    "-in both ADAboost and Gradient Boosting, BOTH the data and the model are changing with each iteration\n",
    "\n",
    "# 6.06 svm\n",
    "-best video to illustrate https://www.youtube.com/watch?v=-Z4aojJ-pdg from 5-14+ min\n",
    "\n",
    "-kernel trick: instead of using b1x1 + b2x2 = y (hence functions are f(x1), f(x2)), kernel trick is to have instead b1x1^2 + b2x1x2 + b3x2^2 = y (hence functions are now f(x1), f(x1,x2),f(x2)), or higher dimensions\n",
    "\n",
    "-kernel choices can be either linear (if fewer rows than cols/features, or if extremely more rows than cols/features. can also use logreg), polynomial, or gaussian (if 10x more rows than cols/features. Not used when too many rows/cols, cos else computationally expensive if very many rows/cols) (=rbf). RBF tends to be best\n",
    "\n",
    "eg. Kernel choices (open table in j notebook)\n",
    "|_____|___  |______________|short and wide: linear\n",
    "|     |     |\n",
    "|     |     |\n",
    "|_____|_____| mid size table: gaussian/rbf, cos else computationally expensive if very many rows/cols\n",
    "|     |\n",
    "|     |\n",
    "tall and narrow: linear\n",
    "\n",
    "-low risk of overfitting (cos u can set a low C. C penalises errors, high C means VERY strict, which leads to overfit. So lower C may be good), resistant to outliers (cos the support vectors which are the 'boundaries touching the different classes' points nearest to the dividing line/decision boundary' would be v far from those outliers)\n",
    "\n",
    "# 6.06 generalized linear models\n",
    "which model to use, depending on continuous/categorical, (-infinity to +infinity)/[0 to +infinity)/ordinal etc? refer to cheat sheet on GA pdf slides\n",
    "\n",
    "-also recap...\n",
    "\n",
    "linear: y = b0 + b1x1 + b2x2 + ... + e, where e is the error\n",
    "\n",
    "logreg: lon (odds) = lon(p/(1-p)) = b0 + b1x1 + b2x2 + ... + error, \n",
    "\n",
    "hence p/(1-p) = e^(b0 + b1x1 + b2x2 + ... + error), \n",
    "\n",
    "hence p = alpha/(1+alpha), where alpha = e^(b0 + b1x1 + b2x2 + ... + error)\n",
    "\n",
    "-generalised linear model: y = g(b0 + b1x1 + b2x2 + ... ) + error\n",
    "\n",
    "-binomial - given 20 trials, how many trials to achieve 14 successes?\n",
    "\n",
    "-negative binomial?\n",
    "\n",
    "-note: When we fit a model in statsmodels, we need to add the column of 1s so that we can have an intercept. statsmodels will not automatically add an intercept for us.\n",
    "\n",
    "# 6.07 gradient descent\n",
    "-difference btw loss and cost function: A loss function is for a single training example. It is also sometimes called an error function. A cost function, on the other hand, is the average loss over the entire training dataset. The optimization strategies aim at minimizing the cost function. https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/\n",
    "\n",
    "-examples of loss functions: MSE (useful), MAE (mean absolute error), gini impurity (rmb? for decision trees)\n",
    "\n",
    "-https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23, https://sebastianraschka.com/faq/docs/cost-vs-loss.html\n",
    "\n",
    "-gradient descent eqn: b1(i+1) = b1(i) - alpha(differentiation: gradient of Loss, wrt to b1), where alpha is the hyperparameter for learning rate. steps to do gradient descent:\n",
    "\n",
    "    1. instantiate model\n",
    "    2. choose alpha\n",
    "    3. choose starting b1(i)\n",
    "    4. find b1(i+1) via gradient descent method above\n",
    "    5. find |b1(i+1)-b1(i)|\n",
    "    6. repeat steps 3 (updated with the new b1(i+1)) to 5\n",
    "    7. stop when a)step 5 hits a low enough threshold, or if b)no. of iterations hit a limit you set\n",
    "\n",
    "-batch gradient descent: update betas/thetas after parsing thru ALL data\n",
    "\n",
    "-stochastic grad desc: update betas/thetas after parsing thru EACH row\n",
    "\n",
    "-mini batch grad desc (the in-between solution): update betas/thetas after parsing thru SOME rows\n",
    "\n",
    "# 8.01 clustering (k-means), unsupervised learning. \n",
    "-https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "\n",
    "-the 'means' in 'k-means' refers to the centroid, which is the mean of all points in a cluster. so k-means refers to k number of centroids. There's also k-medoids which uses median of the cluster not mean\n",
    "\n",
    "-RFM (recency frequency monetary). how recent customer visited, how frequently, how much he spent. These are good features to choose to cluster, in addition to demographics/income etc. Can eventually have a dashboard that allows you to pick features, and see how the clusters change. You can then see which features separate cluster best. CLV/LTV (customer lifetime value, lifetime value) also important too\n",
    "\n",
    "-k-means clustering very affected by outliers, AND also K-Means is sensitive to centroid initialization (see notes). In all, sensitive to shape, scale of data (!!!)\n",
    "\n",
    "-K means (unsupervised) vs KNN (supervised, score by elbow curve/inertia/silhouette)\n",
    "\n",
    "-scoring/evaluation of model by: inter group distance, and intra group characteristics (eg. how tightly packed a group is. if tight, means very distinct, concentrated cluster). score by Inertia, Silhouette score, Homogeneity, Completeness, V-measure. sklearn.metrics has a useful 'homogeneity_completeness_v_measure' function\n",
    "\n",
    "-Homogeneity: a measure of how homogenous a cluster is. each cluster contains only members of a single class. eg. cluster A comprise all apples, cluster B comprise all bananas. BUT what if a separate cluster C comprise all apples too? then we need...\n",
    "\n",
    "-Completeness: all members of a given class are assigned to the same cluster. eg. from above example, cluster A and C both having apples, gotta be tgt in the same cluster\n",
    "\n",
    "-methods: network analysis, topic modelling (use stats model to identify topics/categories in a set of docs. eg. Latent Dirichlet Allocation), clustering, Principal Clustering Analysis\n",
    "\n",
    "-unsupervised learning methods (eg. PCA) can be applied to supervised problems, but the reverse not quite possible\n",
    "\n",
    "-steps:\n",
    "    0. scale dataset (eg. x1 range is [-5,5], but x2 range is [-100k, 100k]), so that high range in 1 variable doesnt impact the analysis\n",
    "    1. Pick a value for k (the number of clusters to create). either randomly, or manually (using domain knowledge), or by Special KMeans++ method in Sklearn (This initializes the centroids to be generally distant from each other), or by k-means elbow method (https://pythonprogramminglanguage.com/kmeans-elbow-method/)\n",
    "    2. Initialize  ð‘˜  'centroids' (starting points) in your data\n",
    "    3. Create your clusters. Assign each point to the nearest centroid.\n",
    "    4. Make your clusters better. Move each centroid to the center of its cluster.\n",
    "    5. Repeat steps 3-4 until your centroids converge.\n",
    "\n",
    "-Inertia: sum of squared errors for each cluster.\n",
    "\n",
    "    - low inertia = dense cluster\n",
    "    - As the number of clusters k increases, inertia decreases.\n",
    "\n",
    "-Silhouette Score [-1 bad to 1 good]\n",
    "    - measure of how far apart clusters are\n",
    "    - high Silhouette = clusters are well separated\n",
    "    - The Silhouette Score is the averge of Silhouette  ð‘   of each point within a cluster in relation to other clusters. The logic behind the solhouette score is that cohesion should be low, while separation should be high:\n",
    "    - cohesion = Average distance of points within clusters\n",
    "    - separation = Average distance of points in one cluster to points in other clusters\n",
    "\n",
    "$$s_i = \\frac{b_i - a_i}{max\\{a_i, b_i\\}}$$\n",
    "\n",
    "    -where:\n",
    "\n",
    "* $a_i$ = Average distance from point $x_i$ to all other points **in the same cluster**.\n",
    "* $b_i$ = Average distance from point $x_i$ to all points **in the next nearest cluster**.\n",
    "\n",
    "-IMPORTANT to be able to present in a visualisation so management can understand, else not explainable is pointless! Hence 3 or max 4 (3d with colour) dimensions at most! but im thinking maybe 5d (time as 5th dim) also ok?\n",
    "\n",
    "-score using silhoutte score, like dbscan\n",
    "\n",
    "# 8.02 dbscan (Density Based Spatial Clustering of Applications with Noise). Superior to k-means (computationally, deciding on no. of clusters, and resistant to outliers)!\n",
    "-instead of deciding no. of clusters, we decide which are the clusters by the hyperparameters epsilon (radius), and minpoints (min mo. of points in that radius). Eg. see https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/, with epsilon 1.00, minpts 4, see the 2nd row, 2nd from right bunch of 5 points (point A topmost, then left-to-right B,C,D, then E bottom most). Notice that all 5 (not 4! the middle point's grey circle is encapsulated within the other 4) are grey, cos they are all considered to belong to same cluster. Cos for point A, it covers the required mininum of 4 points (A,B,C,D) and they all hence belong to Cluster 1. And then for point B, it covers the required mininum of 4 points (B,A,C,E), and all hence belong to Cluster 2. But since A,B,C all belong to Cluster 1 too, hence E from Cluster 2 which is clustered with A,B,C will ALSO belong to Cluster 1. And so on for other clusters. Hence ALL 5 clusters are same grey color.\n",
    "\n",
    "-more efficient than kmeans, due to above point. Cos if we know A,B,C from Cluster 2 are also in Cluster 1, then E would also belong to Cluster 1 - this shortens the computation process somehow\n",
    "\n",
    "-from point 1, this method hence also dispels outliers as they won't be part of any clusters if they don't fall within the epsilon\n",
    "\n",
    "-BUT, if data points are all bunched in a long chain, dbscan may just give 1 big long cluster, whereas k-means would give you your stated no. of clusters. Hence important to EDA first to see if there are long bunched up clusters, or if there are distinct clusters \n",
    "\n",
    "-cos unsupervised (no labelled data, dunno whether its right or wrong), hence cannot/shouldn't do train test split\n",
    "\n",
    "-maybe sometimes we don't need ALL features to do dbscan. Can use pairplot to filter away unecessary features\n",
    "\n",
    "-steps:\n",
    "    1. scale (fit_transform) data. Important!\n",
    "    2. dbscan (fit) \n",
    "    2.5 (good to do). create column 'cluster' to see labelled classes. use 'dbscan.labels_' to see labels. then pairplot, or means. or df.groupby('cluster').mean().T to see the mean of all classes\n",
    "    3. silhouette_score\n",
    "\n",
    "-score using silhoutte score, like k-means\n",
    "\n",
    "# project 3 (web APIs, NLP text classification) feedback\n",
    "-IMPORTANT/DO:\n",
    "    - talk more about EDA, why we EDA a particular way\n",
    "    - what are the key features that'll classify a post to 1 class and not the other\n",
    "    - attempt to use AWS to run code, store database etc\n",
    "    - explaining (a good) business problem\n",
    "    - using multiple models (ensembles of multiple models and vote the aggregate from amongst them, or pick the best from amongst them), and comparing them all\n",
    "    - need to use vectorizers to decide on what are good parameters to start with on gridsearch\n",
    "    - going above & beyond syllabus, which requires extra learning beyond General Assembly\n",
    "    - use OLS stats model to check p-value of each feature, to drop statistically insignificant features\n",
    "    - explain why we use certain metric eg. why accuracy/Area-under-ROC. Show sensitivity, specificity\n",
    "    - ROC (which plots y-axis of true +ve against x_axis of false +ve), business decision is to choose the point where curve starts tapering off. =when the 1-specificity = false +ve rate (y-axis), has a good enough sensitivity (x-axis), and any further drop in specificity = rise in 1-specificity = rise in false +ve rate, won't have any more significant impact on improving sensitivity. Once we hv picked out that elbow where it starts to taper off, use that as your probability threshold, not the usual 50%\n",
    "    - add more customised stopwords (beyond NLTK's) that are relevant to my industry\n",
    "    - show more EDA! dont just skip it and jump into eval of modelling\n",
    "    - show seasonality of words, which words in trend at which point in time\n",
    "    - explain why you use bernoulliNB or multinomialNB etc. Gotta make the ppt idiot-proof, cos mgmt would ask silly qns like \"i saw this other gaussianNB or neural network model. Why didn't you use that?\", and you'd have to defend why you didn't, why your model is more suitable for this context than that model\n",
    "   \n",
    "-BAD:\n",
    "    - SHOULDN'T have code in ppt at all!!! Business people would be deterred\n",
    "    - DON'T show obvious things. eg. showing that Oculus/Vive are strong features for the Oculus/Vive ubreddits, because duhhh, they are representative\n",
    "    - DON'T use red for background in slides cos jarring\n",
    "\n",
    "# 8.03 hierarchical clustering\n",
    "-https://www.geeksforgeeks.org/ml-hierarchical-clustering-agglomerative-and-divisive-clustering/: \"Divisive algorithm is also more accurate. Agglomerative clustering makes decisions by considering the local patterns or neighbor points without initially taking into account the global distribution of data. These early decisions cannot be undone. whereas divisive clustering takes into consideration the global distribution of data when making top-level partitioning decisions.\"\n",
    "\n",
    "-dendrograms: the lower the height of the split, the more similar the splitted categories are. to measure/quantify how well the dendrogram is, use the cophonetic correlation coefficient: 'cophenet()' \n",
    "\n",
    "-viewing it in a topdown approach is Divisive (='D'escending), bottom up is Agglomerative (='A'scending)\n",
    "\n",
    "-expensive algo\n",
    "\n",
    "-1 slice of the dendrogram at a certain height, yields a few clusters (eg. 5), and this is the k-means output if you had defined k to be 5 in k-means\n",
    "\n",
    "-clustering decided by avg/max ('complete linkage')/min ('single linkage')/median ('ward linkage') distance between clusters' points, or dist between centroids ('centroid linkage') \n",
    "\n",
    "# 8.05 recommendation engines\n",
    "-key is to get data on which user engage with what product\n",
    "\n",
    "-CTR (click through rate)\n",
    "\n",
    "-unstructured data (text/image/video), semi-structured data (error logs. the text is the unstructured, the timestamp and category of error is the structured), structured data (eg. ordered rows in excel)\n",
    "\n",
    "-explicit and implicit feedback: for implicit feedback, the static data (age, gender) is waaaay less important than their user behaviour (eg. length of video watch, genres preferred)\n",
    "\n",
    "-recommendations can be based on 2 ways: by features of the product eg. milk that is low fat and organic (content based filtering), or by features of similar users eg. similar past preferences such as both users who used to like linkin park and metallica (collaborative filtering). Content based filtering creates various features for a product, and gives weights or coefficients to them, much like tfidf/logreg\n",
    "\n",
    "-content based filtering: eg. rows are products, columns are features of products (product-vs-product or item-vs-item approach)\n",
    "\n",
    "-collaborative filtering (http://yifanhu.net/PUB/cf.pdf, https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe): user-vs-item approach. eg. rows are users, columns are products. for user 1, see which other users are similar to him as a whole (eg. users 2&3), and if users 2&3 like product A, then we recommend product A to user 1 too. And more than just product A too. So there'd be a \"correletion heatmap\" mapping each user to every other user, and we see who each is most similar to. Or instead of matching users to users, you could match a demographic group who's the 'preference doppelganger' to another user. Scoring could be by the metric: Alternating Least Squares or 'ALS' (a model based approach). \n",
    "\n",
    "A very good application would be recommending near-expiring or unsold/unpopular products to customers who are more likely to buy them. Benefits isnt just clearing expiring/unsold stocks, but also with the ability to sell unpopular products, you'd be able to negotiate for higher profit margin for being able to sell unpopular products (but they may be just as good products, just that major brands dominate the market)\n",
    "\n",
    "-another scoring metric: Jaccard similarity (a memory based, user-vs-user approach), which is no. of elements in (A intersect B)/ no. of elements in (A union B). So, we measure the similarity between users, like KNN - but not by euclidean distance, instead by Jaccard similarity. Here, doesnt use ALS method\n",
    "\n",
    "-another scoring metric: cosine similarity. can call it via \"cosine_similarity()\", or \"pairwise_distances(sparse_pivot, metric='cosine')\"\n",
    "\n",
    "-another scoring metric: Precision of k items (P@k). eg. P@5=0.2 means out of 5 products you recommend, 20% accurately targets the right ppl and they buy\n",
    "\n",
    "-all these metrics, tuning hyperparameters via: give weights to recency of features (eg. more weight to recent songs than old ones)\n",
    "\n",
    "-steps (using j notebook example):\n",
    "    \n",
    "    For new users, do this first. Content based filtering (items are broken down into \"feature baskets\". These are the characteristics that represent the item. The idea is that if you like the features of song X, then finding a song that has similar characteristics will tell us that you're likely to like it as well.)\n",
    "    \n",
    "    1. \n",
    "    \n",
    "    only after you have more info, then use User based collaborative approach\n",
    "    \n",
    "    1. Get Nancy's similarity scores to all other users.\n",
    "    2. Only keep positive similarities. (This step can be tweaked!)\n",
    "    3. Convert Nancy's similarities to weights by dividing each similarity by the total similarity.\n",
    "    4. Get all the other users' ratings for movie X.\n",
    "    5. Weight each user's rating by Nancy's similarity to that user.\n",
    "    \n",
    "    or, use Item based collaborative approach\n",
    "    \n",
    "    1. Get movie X's similarity scores to all other movies.\n",
    "    2. Only keep positive similarities. (This step can be tweaked!)\n",
    "    3. Convert movie X's similarities to weights by dividing each similarity by the total similarity.\n",
    "    4. Get Nancy's ratings for all other movies.\n",
    "    5. Weight Nancy's ratings for all other movies by movie X's similarity to those movies.\n",
    "    \n",
    "-GOTTA DIFFERENTIATE customer! those who know what they're looking for would go straight to searchbar (TFIDF used there to smart guess similar products), but those who dunno what they wanna buy we show them most popular items (cos we dunno what they want), or what ppl similar to him (from cookies, demographics) are browsing at now\n",
    "\n",
    "-should use sparse matrix when only 0.5% of the matrix is filled. Sparse matrix can be compressed sparse col or rows, where only non NaN values are stored to make it 'lite'. https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)\n",
    "\n",
    "-read 5 rules of normalisation. see class_yz_gitga chp 8.05 folder\n",
    "\n",
    "-google with key words: \"collaborative filtering scholar\". \"scholar\" to look for resaarch articles\n",
    "\n",
    "# 8.06 Alternating Least Squares (ALS)\n",
    "-https://engineering.fb.com/core-data/recommending-items-to-more-than-a-billion-people/, https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe, https://github.com/benfred/implicit/blob/master/examples/lastfm.py, https://jessesw.com/Rec-System/\n",
    "\n",
    "-3 million instacart orders (shopping dataset): https://www.instacart.com/datasets/grocery-shopping-2017\n",
    "\n",
    "-We have our original matrix R of size u x i with our users, items and some type of feedback data. We then want to find a way to turn that into one matrix U with users and hidden features of size u x f and one matrix V with items and hidden features of size f x i (f, a hyperparameter, is set by you). In U and V we have weights for how each user/item relates to each feature. What we do is we calculate U and V so that their product approximates R as closely as possible: R â‰ˆ U x V. By randomly assigning the values in U and V and using least squares iteratively we can arrive at what weights yield the best approximation of R. With the alternating least squares approach we use the same idea as \"least squares\" (we compare each element in R against UxV and find the least 'sum of square error' across all elements) but iteratively alternate between optimizing U and fixing V and vice versa. We do this for each iteration to arrive closer to R = U x V. \n",
    "\n",
    "-ALS to recommend user to top N products, OR recommend product to top N users (latter, v useful! eg. if a movie is ending screenplay soon, Netflix would wanna recommend it to ONLY the N likeliest viewers who wanna view it, instead of pestering EVERY customer. Another application is to recommend expiring products, to ONLY customers who are likeliest to purchase it, and not pester EVERYONE with your ads)\n",
    "\n",
    "-ALS should just use just a few features, and will still be powerful. \n",
    "\n",
    "-DON'T tell business ppl/management what your features are, cos they'd chip in thinking they know best but it'd scale up and make processing more complex\n",
    "\n",
    "-when u import implicit, 'implicit.als.AlternatingLeastSquares(factors=50)', the 'factors' refers to the f in the u x f and in f x v\n",
    "\n",
    "-in the code from \"https://github.com/benfred/implicit/blob/master/examples/lastfm.py\" in the para \"# Let's say we want to recommend artists for user with ID 2023\", we eventually get a 1 x n matrix via 1 x f dot product with f x n number of movies. This 1 x n matrix should be as close to the original 1 x n matrix (or row) in the original R matrix, as possible, cos we should have minimised least squares error by now. Now, in the original R matrix, perhaps Jonny had not watched Batman movie before, hence had no rating there, but after this matrices dot product, it'd yield a score for Jonny for Batman movie, which if high and in top 10 amongst all movie scores, will be recommended to Jonny as a similar movie to watch\n",
    "\n",
    "-code here:https://towardsdatascience.com/alternating-least-square-for-implicit-dataset-with-code-8e7999277f4b\n",
    "\n",
    "-UI is important even for recommendation engines. Eg. you dont wanna appear to coerce customer to buy your recommended product. you gotta say smth gentler like \"users who bought A also bought B\", else u scare them off. Also, very important is to deliver the info to the front end, else all your work in the recommendation engine unable to deliver effectively to clients, also no use\n",
    "\n",
    "# 7.01  intro to correlated data\n",
    "-Facebook's Prophet, LSTM, or at least autoarima, are better than ARIMA/SARIMA/SARIMAX. Don't bother trying the latter 3. All the below notes for nth lol\n",
    "\n",
    "-all the EARLIER METHODS we learnt above (big X matrix & y column target), CANNOT BE APPLIED onto time-series, cos theres only 1 feature, plus there's not much target (except future data). \n",
    "\n",
    "-But if die die wanna convert timeseries to big X matrix & y column target, then (not ideal, not usual, not terrific results): you can group the time rows into groups, eg. from rows of by-the-second data, grouped into categories of months, or even years   \n",
    "\n",
    "-spatial correlation, is identified by land areas that are near/in-contact with each other \n",
    "\n",
    "-parametric data requires independance between features (certain assumptions are made of the data, eg. normal), non-parametric doesnt make assumptions (eg. decision trees)\n",
    "\n",
    "-if latest row/observation is dependant on the previous data, then it is correlated to previous data. Not independant!\n",
    "\n",
    "-auto-correlation: the 'auto' means, by itself, much like auto in auto-biography. Hence it's self-correlation - we're finding correlation of the feature UNTO ITSELF (=current vs past. Which is the reality for eg. engine data, stock data, where past data can affect present and future data), unlike in heatmap where we check heatmap for correlation BETWEEN features\n",
    "\n",
    "-to model time dependance, we could use...\n",
    "    - AR equation: y(t) = b(0) + b(1)y(t-1) + b(2)y(t-2) +... + b(k)y(t-k) + epsilon(t) (which is the 'lag'), where b's are the coefficients, and order=k=lag=p. So if p in AR(p=1), then eqn has beta stopping at 1: y(t) = b(0) + b(1)y(t-1) + epsilon(t). These betas will be the coefficients from the output of \"tsa.statespace.ARIMA() or SARIMAX() etc.fit().summary()\" in chp7.03 solution codes \n",
    "    - MA equation: y(t) = miu + epsilon(t) + theta(1)epsilon(t-1) + theta(2)epsilon(t-2) + ... + theta(k)epsilon(t-k), where miu is average, theta's are the coefficients, and order=k=lag=q. So if q in MA(q=1), then eqn has theta stopping at 1: y(t) = miu + epsilon(t) + theta(1)epsilon(t-1). These thetas will be the coefficients from the output of \"tsa.statespace.ARIMA() or SARIMAX() etc.fit().summary()\" in chp7.03 solution codes\n",
    "    - for ARMA, simply add both eqn tgt\n",
    "\n",
    "-to model spatial dependance, eg. for elections, focus on correlation between areas where adult population is high?\n",
    "\n",
    "-for network analysis, nodes are connected by edges (the link/relationship). Together they form a graph. The edges can have arrowheads to indicate that they are uni/bi-directional\n",
    "\n",
    "-DAG \"Directed Acyclic Graph\": a circle of nodes in which you can't follow the edges' directions and go full circle cos you'd hit a reverse direction. The opposite is DCG \"Directed Cyclic Graph\", where you can follow the edges' arrowheads' directions and go 1 full circle. \n",
    "\n",
    "A node represents a state, arrow represents flow of data. eg. node A undergoes a computation to go to state B, undergoes another computation to go to state C. But C cannot undergo a process to reach state A again (else that would be cyclic). Note that tensorflow use DAG\n",
    "\n",
    "-for time series, MUST make data stationary (stationarity is one whose statistical properties such as mean, variance, autocorrelation = correlation of t with itself at t-1, covariance of t with itself at t-1 etc. are all constant over time). Done by using differencing: df.diff() to find the difference btw every row n and row n-1. Sometimes might need to do 2nd order, or 2nd round of differencing. KEEP DOING until the statistical properties are constant over time (can verify with diagnostic tests using...). To check for stationarity, use the Dickey Fuller test (good function found in GA chp7.03 ARIMA-AIC Forecasting solution-code). If fail, do differencing, repeat till Dickey Fuller test succeeds, and use that final differenced version. For white noise (likely for machinery), even after multiple differencing, still won't be stationary. If after 1 round of differencing, all lag values are not statistically significant, we end here and conclude its all white noise\n",
    "\n",
    "-to apply \"1-lag differencing\" = apply 1 round of differencing\n",
    "\n",
    "-Auto-Regression (AR), Moving Average (MA). ARIMA is Auto-Regression Integrated with Moving Average, = ARMA with differencing\n",
    "\n",
    "-from seasonal_decompose function: trend is well, the trend. Seasonal is the regular pattern. Residual is the error, and it SHOULDN'T have trends, should be random (use Ljung-Box test to check residuals. Its only for time series data). In the seasonal_decompose function, By default, the decomposition will assume there is a linear trend. If you believe there is an exponential trend, you should change the argument model to be multiplicative, but this will fail with values that are less than or equal to 0.\n",
    "\n",
    "-acf_plot & pacf_plot: anything that is outside of shaded area (the 95% confidence interval), it is statistically significant (cos its assumed to be so unlikely to happen, yet it happened). If within, its insignificant. ACF considers correlation btw t and t-1 and all other past observations (long term view), but PACF filters away t-2 and earlier, hence only considers correlation between t and t-1 (cos t-2, t-3 may have had an influence on t-1) (short term view). Hence ACF is better for long term analysis\n",
    "\n",
    "-ACF btw t0 and t-2: assume t-2 and older (t-3, t-4 etc) influences results, but exclude t-1 (cos t-1 is still 'unknown' at t-2, can't be used to predict t0)\n",
    "\n",
    "-PACF btw t0 and t-2: assume only t-2 influences results, but exclude anything older (t-3, t-4 etc), and t-1 (same reason as above)\n",
    "\n",
    "-in ACF & PACF plots, 1st leftmost bar at t=0 is always height =1, cos its correlation of y(t) with y(t) itself (much like correlation heatmap's diagonal of 1's, the corr. of x(i) against itself =1, duh)\n",
    "\n",
    "-For ACF, PCAF, when its human related data, try to look out for k or lag=12 or multiples of 12 cos things might happen yearly\n",
    "\n",
    "# 7.02 ARIMA\n",
    "-4 categories of time series models. The p (order of AR term)/d (no. of rounds of differencing needed to make time series stationary)/q (order of MA term) MUST stick with AR/MA as shown below, and MUST be in the sequence seen below. p,d,q are ALWAYS integers, and uncommon to have large values:\n",
    "    - AR (p)\n",
    "    - MA (q)\n",
    "    - ARMA (p,q)\n",
    "    - ARIMA (p,d,q). If you've tried differencing at all, even if optimum results is d=0, should still indicate as ARIMA(p,0,q), not ARMA (p,q)\n",
    "    Note that ARIMA (p,0,q) = ARMA(p,q). Also, p/q can also be 0, eg. if p=0, means theres no AR. ARIMA(p=1,d=0,q=0) = ARMA(p=1,q=0) = AR(1)\n",
    " \n",
    "-use PACF to determine p (for AR. simple way to rmb is 'P'ACF for p), use ACF to determing q (for MA), Dickey Fullers test to determing whats a good d (no. of differencing) to stop at\n",
    "\n",
    "-Identifying the order of differencing in an ARIMA model: https://people.duke.edu/~rnau/411arim2.htm\n",
    "\n",
    "-steps:\n",
    "    1. use ACF/PACF plots to decide whether to use AR/MA/ARMA/ARIMA\n",
    "    2. identify coefficients & model performance factors (eg. error terms)\n",
    "    2.5. if multiple models work, use the one with least error, where error metric is AIC (ignore BIC), where lower is better. Also, choose the simplest (= lowest order p in PACF, or lowest order q in ACF. Basically, choose the lag value on the LEFTMOST of the ACF or PACF graph) model. If we see that its underfitting, then we choose the next higher order p or q which is statistically significant, albeit its less simple\n",
    "\n",
    "# 7.04 advanced time series (auto-arima)\n",
    "-for time series, very good to: pip install pmdarima, to get \"autoarima\", then \"from pmdarima.arima import auto_arima\" which is super powerful! DON'T NEED ACF, PCAF le\n",
    "\n",
    "-even better is facebook's Prophet library!\n",
    "\n",
    "-for time series usually we dont do train test split. Instead, set train as \"train = data.loc['1985-01-01':'2014-12-01']\"\n",
    "\n",
    "-autoarima can be used for arima, sarima, sarimax\n",
    "\n",
    "# 7.05 spatial data analysis\n",
    "-rows/observations are not independant from each other, unlike typical datasets\n",
    "\n",
    "-if we have 3 regions A,B,C, eqn for impact on each of these regions are:\n",
    "    - y(A) = beta(A) + beta(A,B)y(B) + beta(A,C)y(C) + D, the non-spatial/temporal features\n",
    "    - y(B) = beta(B) + beta(B,A)y(A) + beta(B,C)y(C) + D, the non-spatial/temporal features\n",
    "    - y(C) = beta(C) + beta(C,A)y(A) + beta(C,B)y(B) + D, the non-spatial/temporal features\n",
    "\n",
    "For smog example in slides, its smog(A) = rho x W(A,B) x smog(B) + rho x W(A,C) x smog(C) + D, and smog(B) & smog(C) eqns\n",
    "\n",
    "-spatial auto regressive model: y(i) = rho x summation from j to all, of [W(ij) x y(j)]. Where W is a known weight between i and j, and y(j) are other values of y that i want to predict. y(i) is what i wanna predict. rho is to be computed/optimised\n",
    "\n",
    "-read \"curse of (high) dimensionality dataset\". Effective solution for dimensionality reduction, is PCA\n",
    "\n",
    "-use Moran's I Statistic to determine if there's spatial autocorrelation (=regions are distinctly separated. score is +1, has perfect spatial correlation)\n",
    "\n",
    "-when in doubt, defer to non-parametric (no assumptions that the data has any underlying distributions, eg. normal dist) hypothesis tests. If you are working with \"irregularly placed data gatherers\", this might be v helpful\n",
    "\n",
    "-train test split: when picking test set, choose datapoints that are in locations where there are also quite some datapoints from train set too (instead of choosing an outlier as test set). Also choose test set datapoints in an evenly distributed way, from a mix of such locations, not all just from 1 location  \n",
    "\n",
    "-for spatio-temporal data, problematic if we have differing reporting periods. eg. col 1 is no. of bikes rented, data collected by the hr. col2 is weather info, data collected by 6hr. But if from hr1-5, it's Rainy throughout (when it truly isn't), then data analysis would be inaccurate\n",
    "\n",
    "# 7.06 network analysis\n",
    "-in 7 Bridges of Konigsberg: when you convert into node diagram, the land mass/length of bridges don't matter. Important is just let land masses be nodes, bridges be links. Condition for being able to travers: a) each node/land mass must hv even degree/no. of links or b) exactly 2 nodes must have odd degree/no. of links\n",
    "\n",
    "-check out setosa markov chains: http://setosa.io/blog/2014/07/26/markov-chains/\n",
    "\n",
    "-check out a peculiar, graph-based database (instead of typical row/col databases): https://neo4j.com/\n",
    "\n",
    "-connectedness: how info spreads thru; average degree of a network: mean number of connections of a node; degree distribution of network: set of all values of a variable and how frequently we observe them being right-skewed\n",
    "\n",
    "-adjacency matrix, when A is connected to B, B connected to A & C: [[0 1 0],[1 0 1],[0 1 0]], or 1st row [0 1 0], 2nd row [1 0 1] etc, where 1st/2nd/3rd rows/cols are for A/B/C respectively\n",
    "\n",
    "-isomorphic: 2 graphs are isomorphic if they are the same graph nbut just \"twisted/drawn\" differently. eg. China's zui qiang da nao challenge, where players have to \"unravel all nodes from a network diagram\" so that no links overlap each other \n",
    "\n",
    "-difficulties with network analysis: a) sampling is hard and will have bias (can try forest fire sampling, or snowball sampling, but got limitations); b) hard to visualise (eg. isomorphic graphs)\n",
    "\n",
    "# 8.04 intro to PCA\n",
    "-to reduce dimensions for df with high dimensionality (more cols than rows). It'll draw a best-fit line (or vector) that spans (doesn't need to touch) the most number of datapoints\n",
    "\n",
    "-PCA is NOT to improve score! Its to avoid curse of dimensionality!\n",
    "\n",
    "-MUST do standard scaling first, then PCA\n",
    "\n",
    "-suitable ONLY for numerical, not categorical cols\n",
    "\n",
    "-can use PCA straight, skipping all the VIF/heatmap corr checks/VarianceThreshold\n",
    "\n",
    "-Principal Component 1 (PC1), is the eigenvector that explains the feature w the largest variance. It has the largest eigenvalue (=the magnitude of the vector while spanning the datapoints). PC1 alone would have explained most of the variance already (must be >=60% at least) Next, PC2 must be orthogonal & in-plane to PC1 (which hence means PC2 possibly explains much less variance). Computer can generate even more PCs (eg. PC3 is out of paper). We \".fit\" on these PCs. How many PCs to generate, is subjective\n",
    "\n",
    "# 7.07 benfords law\n",
    "-its basic. lol\n",
    "\n",
    "# extra sharing: weekiang's ppt on Deploying ML models as Microservices\n",
    "-heroku doesn't scale with large requests - can't scale. Also Load balancing issues, so have to create more instances, and distribute requests over a larger number of servers. Also, eg. in a superapp which has many services, if you only deploy on 1 server, whole app will fail - single point of failure\n",
    "\n",
    "-machine learning is only <5% of production systems. A bulk of the work is in Serving Infrastructure, Datacollection, configuration, monitoring, machine resource management etc\n",
    "\n",
    "-A Docker app (which is a containerized application, a Microservice) is better than a Virtual Machine, uses less ram, no worries of different Operating Sys (eg. hence can run Apple app on Windows OS. It doesn't have an OS, really. Without an OS, a Docker app/containerised app hence doens't consume much RAM nor harddisk space). You install a Docker image, which is a clone of the original master copy. It solves the issue of a shared software being able to work on any OS/machine/hardware. So GrabFood might have many microservices running behind the scenes. With multiple microservices, there's no single point of failure\n",
    "\n",
    "-It structures an app as a collection of losoely coupled apps. It allows: a) can handle separate biz functions, instead of 1 big program; b) several smaller applications, c) communicate via well defined APIs (usually http)\n",
    "\n",
    "-manage your containers, using Kubernetes\n",
    "\n",
    "-flask on heroku is relying on monolithic architecture, quite dangerous cos single pt of failure. Better to deploy as microservices\n",
    "\n",
    "-steps:\n",
    "    1. build ML model, create web front end using flask\n",
    "    2. build docker container on local\n",
    "    3. push doker container onto cloud eg. AWS\n",
    "    4. create kubernetes cluster (\"brew install kubectl\" in cmd line, \"brew cask install minikube\" etc, \"minikube start\"). Create YAML file to deploy app, deploy YAML file to kubernetes cluster\n",
    "    5. deploy containers\n",
    "    6. expose deployment as service, by creating a service object\n",
    "    \n",
    "# hard truths from weekiang\n",
    "-competitors: uni DS/analytics grads (bach/masters) where 1st batch graduated in 2019, METIS sg DataScience bootcamp alumni (1st batch grad in sept 2019. But 7/18 in 1st batch dropped out even tho METIS had strict intake criteria. 50% of 11 hired 3mths after grad), AI sg AI apprentice alumni, other DSI batch, Jnr Datascientists, sw developers w strong skills, and more. At this point dec2019, there are >300 competitors\n",
    "\n",
    "-high supply of junior DS, v limited supply of snr DS. Hirers want top tier DS (heads, leads, snr DS), can pay 5fig 10k salary\n",
    "\n",
    "-make sure your resume has keywords found in JD, especially bad if outdated HR ppl filter. Applicant Tracking System \n",
    "\n",
    "-for job hunt, Spray and pray! \n",
    "    - apply widely & daily for jobs, be thickskin and apply widely even tho JobDescription may not be what you think you fit (eg. require Masters in Datascience), cos the company may not even be able to find that unicorn hire that they seek, and they may just take the next best who doesnt have the Masters. \n",
    "    - Dont be choosy.\n",
    "    - prev exp and salary may be irrelevant\n",
    "    \n",
    "-Job hunt will be EXTREMELY tough. DREAM SALARY IS A DREAM. ADVANCED SQL hands on skills VERY IMPORTANT (eg. complicated joins is expected knowledge), may be more important than machine learning (regardless data analyst/scientist interview, will be tested). Hirer will not be kind, will doubt you, why hire you rather than a bach/masters grad with multi years exp. There will be on-the-spot code challenge\n",
    "\n",
    "-stay active on LinkedIn:\n",
    "    - be useful\n",
    "    - be very responsive on invites/messages (within 24hr!!)\n",
    "    - hv a public linkedin pg\n",
    "    - share your DS projects\n",
    "\n",
    "-Build a comprehensive DS portfolio, publish actively your DS projects (eg. on TowardsDataScience, AnalyticsVidhya, Medium). Post linkedin/portfolio url on either \n",
    "   \n",
    "\n",
    "# 10.01 deep learning\n",
    "-https://towardsdatascience.com/introduction-to-artificial-neural-networks-ann-1aea15775ef9 , https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/ , https://towardsdatascience.com/inroduction-to-neural-networks-in-python-7e0b422e6c24\n",
    "\n",
    "in the perceptron diagram, in hidden layer, we call the cells/neurons A0/A1 etc, where A stands for 'Activation function', as it takes in the prior layers' features, gives weights to them, and if threshold is exceeded, will Activate/Turn off (it'll either be True/False. Simply 1/0) giving an Output\n",
    "\n",
    "-from pixels, we feature engineer to lines in the next hidden layers, then ft engr to curves in thereafter hidden layers\n",
    "\n",
    "-2 categories: CNN, RNN (eg. LSTM)\n",
    "\n",
    "-neural networks, start with 1 hidden layer, cos it can already work well\n",
    "\n",
    "-Perceptron: when we say 1 layer neural network, that 1 single layer is the output layer\n",
    "\n",
    "-activation functions: sigmoid, tanh, ReLu/Leaky Relu\n",
    "\n",
    "-rule of thumb: neurons increase then decrease in no. frm input to output (no. of neurons in output, should be the same no. of classes that you wanna classify into)\n",
    "\n",
    "-softmax is like sigmoid but for multi-class classification \n",
    "\n",
    "-https://medium.com/ai%C2%B3-theory-practice-business/dropout-early-stopping-188a23beb2f9 . \n",
    "\n",
    "-minibatch size (or \"batch_size\" in keras library) typically should be in multiples of 2, cos that works best with the inner workings of GPUs.\n",
    "\n",
    "-hidden layers should use ReLU (way better than sigmoid), final output layer can use sigmoid/tanh (binary classification), or softmax (multiclass classification)\n",
    "\n",
    "-dont add too many neurons eg. 512, it'll burn your machine\n",
    "\n",
    "-even if its a linear regression problem you are solving, also can use neural network. It converts the z=sigma(w1.x1)+b, into non-linear, via the activation function Ïƒ(z) which could be sigmoid\n",
    "\n",
    "-Multi-Layer Perceptron (MLP) has hidden layers\n",
    "\n",
    "# 10.2 keras\n",
    "-keras high level, easier to use for beginners. Tensorflow (by google), Pytorch (by fb) more low level\n",
    "\n",
    "-each 'Dense' you add, you're adding a hidden layer\n",
    "\n",
    "-input>hidden>output is considered a 2 layer neural network, cos we dont consider input layer as a layer.\n",
    "\n",
    "-if you have 10k rows, 705 weights, and assume only 1 epoch... stochastic (very computationally expensive): trains row-wise 10k times, then updates ALL weights at 1 go after training that row, but repeat for every row (=updates 705 weights 10k times); batch (v fast but less accurate): takes all 10k rows and trains them all at 1 go, then updates ALL weights at 1 go (=updates 705 weights 1 time);  mini-batch (best balance): takes eg. 100rows from 10k rows, trains on 100rows as a batch and updates ALL the weights at 1 go, then moves on to the next 100rows, and we repeat this 100 times till 100sets of 100rows (=10k rows) have all been trained (=updates 705 weights 100 times), and by now weights should all be good. \n",
    "\n",
    "For this example, the batch_size argument in keras, can be 1 (stochastic), 10k (batch), or eg. 512 (minibatch). batch_size MUUUUUST keep be a power of 2!!!\n",
    "\n",
    "If epoch is now 10, then weights get updated 10x more times \n",
    "\n",
    "-loss functions for... binary classification: use binary_crossentropy; multiclass classification: use categorical_crossentropy\n",
    "\n",
    "-for optimizer, good rule of thumb is to use 'adam'. https://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "-steps for keras:\n",
    "    0. Standard Scaler. Scale your data for ANY model that uses Gradient Descent, which includes Neural Networks.\n",
    "    1. create architecture. Specify model, no. of layers/neurons, final layer's activation function (sigmoid for binary classification, softmax for multiclass classification)\n",
    "    2. specify optimization technique. loss(eg. mse), optimizer (eg. adam), metric (eg. mae)\n",
    "    3. Train/fit, and validate. Specify epoch, batch_size (determines whether its stochastic/batch/minibatch)\n",
    "\n",
    "# 10.3 regularizing neural networks\n",
    "-with more neurons in the hidden layer, the neural network can identify more possible patterns. eg. when identifying straight lines, if 32 neurons, this layer can now identify 32 lines at different orientations, but if 64, it can identify even more (64) possible orientations, which allows the neural network to learn more features better\n",
    "\n",
    "-neural network regularization, L2 is preferred over L1\n",
    "\n",
    "-if underfit, reduce lamba to increase weights (=increase learning rate). if overfit, reverse is true\n",
    "\n",
    "-dropout creates different network structures. It 'kills' some neurons, to eventually yield a more generalizeable model. https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab. By killing some neurons (=features) in different models occasionally during training, whereby some of these features have too strong an influence on the prediction, you can hence generalise better (cos sometimes these particular features aren't found in all scenarios. And if you are strongly biased by these features, when a True scenario appears but lacks these particular features, model will wrongly predict as False).\n",
    "\n",
    "Basically, the neural network is disincentivized from allocating too much power to any one weight. It has a similar effect as imposing an L2 penalty: the magnitude of our weights shrinks.\n",
    "\n",
    "Best practices:\n",
    "    - Don't set any keeping probabilities for layers you where you don't want to drop any nodes. (What might be examples of these layers?)\n",
    "    - You'll generally want to specify a single keeping probability on all the layers on which you want to apply dropout, instead of specifying different keeping probabilities for each layer.\n",
    "    \n",
    "-dropout is useful for eg. highly inbalanced datasets. If very small number of positive class is present, they may only have some features which are correlated to the target, but in unseen data there could be other indicative features. Hence we use dropout, to avoid over fitting to those few indicative features, by killing/deactivating some of those neurons/features during learning\n",
    "\n",
    "-early stopping: stop training when change in test/validation (NOT training!) loss goes below a threshold. Cos if we continue training, val loss can actually start rising (=overfit). Goal is to avoid getting stuck in plateau, or saddle point\n",
    "\n",
    "-in Sequential, when you compile, you may choose to not include a metric. In that case, loss will be used as 'metric'\n",
    "\n",
    "# 10.4 Convolutional neural network CNN\n",
    "-how to detect face/lines etc in image: in a 6x6 image, code scans within a 3x3 box, from left to right, top to bottom, to find the face/line etc\n",
    "\n",
    "-CNN architecture is generally: conv-maxpool-conv-maxpool-fullyconnected-fullyconnected-fullyconnected...\n",
    "\n",
    "-increasing 'stride' allows you to speed up training, cos fewer parameters\n",
    "\n",
    "-in 3rd from last slide (https://git.generalassemb.ly/DSI-SG-11/classes/blob/master/10.04-lesson-cnn/slides/cnn.pdf), blue table, under Shape column, the 3rd dimension is the no. of filters you use. Also notice that from conv1 to pool2 layer, assuming we're using a 2x2 window for max pooling, hence 1st and 2nd dimensions (length, width) got halved. Also notice that from pool1 to conv2 layer, assuming we're using a 5x5 window to scan across/convolve, hence 14-5+1=10 for 1st and 2nd dimensions \n",
    "\n",
    "-CNN benefits over ANN: efficient (cos it utilizes maxpooling, hence fewer weights/parameters), allows for spatial referencing \n",
    "\n",
    "# 10.06 intro to tensorflow\n",
    "-follows the DAG concept\n",
    "\n",
    "-tf 2.0 has made many functions in tf 1.0 incompatible. If wana stay compatible w tf1.0, after importing tensorflow as tf, gotta include these 2 lines:\n",
    "\n",
    "    import tensorflow.compat.v1 as tf #these 2 lines is to allow the imported tf (now v2.0) to be compatible w v1\n",
    "    tf.disable_v2_behaviour()\n",
    "\n",
    "-can check version via tf.\\__version__\n",
    "\n",
    "-MUST run a session, then execute computation within a session\n",
    "\n",
    "-the point of lazy eval, is to save computational resources. Cos for big data, we may not need to run ALL lines of the code, instead we wanna run code step by step\n",
    "\n",
    "# 11.01 sql 1\n",
    "-companies store data in different databases, eg. customer info, merchant info, transaction info etc separately. Else if all combine into 1, very messy. \n",
    "\n",
    "-Companies would need to find answers to biz questions, hence need data analysts. \n",
    "\n",
    "data lake stores raw, unstructured data. database stores structured data, which gets extracted into data warehouse. SQL can be used to access all 3 areas.\n",
    "-datalake (Amazon S3): stores raw, unstructured info. eg. click/scroll/view duration/speech/video/images/text data, not the structured, explicit data. It follows an Extract-Load-Transform (ELT) process: you extract raw info, load it, dont transform/process it until such a time you need to. How to query such unstructured data? Note that actually, its stored in a structured way (which is, they're indexed by time. eg. Year, Mth, Day, Hr etc. Can also be split into eg. mobile/Apple phone/Samsung phone/desktop access, to identify customer behaviour) for ease of extraction. Very important for Data Scientists, cos this is where much precious data is stored\n",
    "\n",
    "-database: structured, explicit (eg. transactions) info get stored real time stored into db. OLTP transactions\n",
    "\n",
    "-data warehouse (eg. Google Big Query, AWS, Snowflake, Redshift): too much info stored in database. Hence extract specific info periodically in batches (batches of incremental data. eg. extract db's info from 8-10am at 10am, then extract info from 10-12pm at 12pm. Process them each time they're extracted, then load into datawarehouse. This string of process is called ETL - Extract, Transform, Load. We DON'T extract in info real time, cos eg. 1 million rows coming into db every sec, you can't possibly process the db's info real time cos its live. Will overstress the db system), from database, merged (but uncleaned), to work on for Online Analytical Processing (OLAP). This extraction is done via a software. But data in db usually not well cleaned, staff dont bother cleaning, hence hard to use to answer biz questions.\n",
    "\n",
    "also note that database is smaller than data warehouse, cos after merging multiple databases (eg. product, transactions, customers' db), it'll be formed from multiple combinations of the different databases. You'll get even more columns, even more rows if eg. 1 customer bought many products across many transactions \n",
    "\n",
    "-Entity Relationship Diagram (ERD) https://www.smartdraw.com/entity-relationship-diagram/\n",
    "\n",
    "-typical code is: SELECT 'whichever thing/name/price (all these column names gotta be typed out exactly as it is)' FROM 'whichever table'; (must add semicolon). eg. SELECT foods.name FROM foods; SELECT * FROM foods; (asterisk is wildcard, here means select all columns from the 'foods' table). see here for some syntax: https://git.generalassemb.ly/DSI-SG-11/classes/blob/master/11.01-lesson-sql_I/sql-lecture.pdf\n",
    "\n",
    "-really try NOT to use .\\*, eg. foods.\\*. Cos very expensive operation\n",
    "\n",
    "-no. of queries on SQL db has a limit. If you exceed, you may be temporarily barred\n",
    "\n",
    "-steps:\n",
    "    1. EDA to determine which are the primary keys (PKs. Indexes which are unique across rows, no duplicates in 1 table) in whichever table. Useful to know where these keys are also found in other tables (would be as FK - Foreign Keys, whereby they can have duplicate rows)\n",
    "    2. \n",
    "\n",
    "# 11.02 sql2\n",
    "-why do we use sql, and not just load onto j notebook? cos 1. sql database can hold HUUUGE amounts of data which would otherwise crash j notebook, 2. which means also will take long time to load on j notebook\n",
    "\n",
    "-steps to download postgresql, and how to get it started:\n",
    "    1. gotta first install postgres for windows! preferably install the 2nd newest version, cos newest may be buggy. Download from  https://www.enterprisedb.com/downloads/postgres-postgresql-downloads . You'll be prompted for pswd, so input. Dont need install stack builder when they ask\n",
    "    2. run the exe, download\n",
    "    3. open SQL Shell, press 'enter' (~4times) until you are prompted for pswd. input what you had set above (it'll be hidden from view). You'll end up with a Warning, and finally 'postgres=#' appears\n",
    "    3a. you may wanna install a postgresql IDE/client. eg. dbeaver, tableplus (for both mac and windows), postico (for mac) are good options\n",
    "    4. for the postgres j notebook codealong (which requires a titanic db), we do this...\n",
    "    4a. type: 'CREATE DATABASE titanic'\n",
    "    4b. type: '\\c titanic', to connect to the 'titanic' database \n",
    "    4c. with connection established, we can access titanic database! we could equally access postgres database. (you could actually see the tables you've created, by opening an SQL IDE eg. dbeaver, tableplus (for both mac and windows), postico (for mac))\n",
    "    \n",
    "    (if you cant see the database in dbeaver, try in dbeaver: right click 'PostgreSQL - postgres' -> 'Edit Connection' -> select 'PostgreSQL' tab -> check 'show all database')\n",
    "\n",
    "# 11.03 scala\n",
    "-https://searchbusinessanalytics.techtarget.com/definition/big-data-analytics\n",
    "\n",
    "-Why bother working with big data on cloud? Cos got problems with processing big data locally on laptop: 1. memory (if laptop ram 4gb, daset 3gb, then opening dataset will be v slow. An unscalable solution is to increase ram, but cannot increase indefinitely cos eg. a 4096 GB ram laptop is crazy expensive vs 256x 16 GB ram computers, hence alternatively, do processing in a distributed manner = parallel processing. That said, in parallel processing, it takes time to split data to process, then recombine results), 2. storage (laptop maybe not enough hard disk storage), 3. speed (laptop's CPU may be running only single thread processing hence slow. Also, hard disk is slower than SSD)\n",
    "\n",
    "-Scala is a programming language. Spark is the application, its a parallel processing framework that enables users to run large scale data analytics across clustered systems (v useful for recommender systems). Spark typically uses Scala programming language, PySpark uses python for Spark, RSpark uses R on Spark. Scala is more efficient/faster than PSpark\n",
    "\n",
    "-in databricks' Scala, each time u finish running a line of code, you'll see eg \"(2) Spark Jobs\", \"Job 25\" with a dropdown arrow. Expanding the arrows, you can see multiple jobs having been carried out, and they have all been done via parallel processing on distributed computers!\n",
    "\n",
    "-consider using spark/pyspark/Rspark, if big data tens of GB. Eventually compress/zip it using Apache Parquet, to export out\n",
    "\n",
    "-data has been split and distributed to each computer. Functions in Code is then mapped (=distributed) to each computer to do parallel processing (cos code is easier to be distributed than data), then recombined back into 1 combined result (=reducing). Hence, we call this sequence of steps 'map and reduce'. \n",
    "\n",
    "-Kafka is used to split data that's being streamed in, to do parallel processing. \n",
    "\n",
    "-Hive is used to split database, to do parallel processing. Its an open source data warehousing system, for querying and analyzing big data stored in Hadoop files\n",
    "\n",
    "-it used to be we split data row-wise (eg. chunks of customers, or transactions). Alternative is to split column-wise, useful for sparse matrix (eg. 1 customer has very few interactions with all present features)\n",
    "\n",
    "-SQL Database is a Relational Database and a structured one, whereas NoSQL (https://en.wikipedia.org/wiki/NoSQL) is a Non-relational database likely to be more document and distributed than structured (eg. columnar databases, graphical database, unstructured documents). BOTH are important, but can focus on SQL first. In NoSQL, you're storing every (unstructured) document as 1 row, and each row is a key\n",
    "\n",
    "-NoSQL vs relational databases https://www.mongodb.com/scale/nosql-vs-relational-databases . 1 example of benefit for NoSQL, is eg. you used to have only 5 cols for your data. Following year, 10 new columns get introduced. With relational db, its hard to meld it in cos data schema is fied, but with NoSQL db, you cann still add cos NoSQL is flexible (NoSQL databases donâ€™t require any predefined schema)\n",
    "\n",
    "-on AWS: EMR (elastic map reduce), EC2 (elastic cloud compute). With EMR, it already comes pre-installed with Hadoop etc other software that aids you in parallel processing etc, so dont manually go install!\n",
    "\n",
    "-steps to create on AWS an EMR cluster (not on free tier! Amazon Sagemaker is free though):\n",
    "    1. search for EMR\n",
    "    2. 'create cluster', Software Configuration: choose one with Spark and Hadoop at least. Hardware Configuration: prefer 'Memory Optimized' or 'General Purpose'. The rest, go with default\n",
    "    2a. you can customize by \"go to advanced options\". Gotta check the boxes of at least 'Hadoop' and 'Spark'\n",
    "    3. Choose your EC2 key pair\n",
    "    4. Create Cluster\n",
    "    5. On the 'Cluster: My Cluster' page, Visually verify that it has my required applications installed (eg. Hadoop, Spark)\n",
    "\n",
    "-to save notebook on DataBricks (used to write scala notebook), File->Export->html\n",
    "\n",
    "-For Spark...\n",
    "    - its very computationally expensive to do .show(). Cos from the distributed data, algo needs to recombine them all to show \n",
    "    - nullable columns can contain nulls\n",
    "    - '&lt' means less than, '&gt' means greater than\n",
    "\n",
    "-for machine learning to run on cloud fast, use Decision Trees cos they run fast \n",
    "\n",
    "# 11.07 Docker on AWS\n",
    "-https://www.freecodecamp.org/news/docker-simplified-96639a35ff36/\n",
    "\n",
    "-important for code to be reproducible/same outcome across everyone's computers. Else hampers collaboration, which impacts deployment/revenue\n",
    "\n",
    "-if everyone in team has computers with different OS, they can all agree on a common OS (eg. Windows) for their Docker Host, and they will all build Docker Containers (containing apps/images) atop it. These containers can only work on 1 OS, which is what they have chosen - the Windows Docker Host\n",
    "\n",
    "-VM vs Docker: for Docker you just need to install the minimum required software that you need in it, and it can be used on different OS. Very important when prototyping a new software/product, cos you gotta state upfront what requirement/packages you need in the final product, but IT team won't allow you to bloat it up with unnecessary software\n",
    "\n",
    "-key is to deploy projects on server in the cloud, the more you do, the more valuable you are.\n",
    "\n",
    "-why we break code up into multiple segments/apps/images (eg. create architecture, fwd propagation, back ppgn, train etc): 1. ease of troubleshooting, 2. can deploy each app/image on different servers to increase robustness against failure\n",
    "\n",
    "-steps:\n",
    "    1. run ec2 instance as per above, access via Putty.\n",
    "    2. to run docker on EC2, follow this step by step https://hackernoon.com/running-docker-on-aws-ec2-83a14b780c56 (and do this if permission got denied: https://www.digitalocean.com/community/questions/how-to-fix-docker-got-permission-denied-while-trying-to-connect-to-the-docker-daemon-socket , do up to 'sudo usermod -aG docker $USER' , where USER is probably ec2-user@) If cannot do 'docker ps' or 'docker info', logout then relogin, as follows...\n",
    "    \n",
    "     type 'exit' in putty's ec2 cmd prompt to quit, then re-access ec2 via putty as in step 1. This time, typing 'docker ps' or 'docker info' will work. In class we did up to step 2 (from 1st link, the one from hackernoon). \n",
    "    3. to rn j notebook (eg. scipy-notebook. could be other notebooks as per shown in GA codealong eg. tensorflow-notebook etc): type in ec2 putty cmd prompt 'docker run -p 8888:8888 jupyter/scipy-notebook'. Now our server is up\n",
    "    4. to access your notebook, and if you've chosen to run an AWS Linux ec2 instance, open a NEW git bash terminal and type: 'ssh -L 9999:localhost:8888 ec2-user@[public-dns from your EC2 dashboard]' (this is port forwarding). The original ec2 putty cmd prompt is occupied, its running the j notebook, just like how whenever we open a new j notebook, a new terminal would appear as a new running window. (thereafter We ctrl-c to close ec2 putty and j notebook, to continue w next part of class)\n",
    "    \n",
    "    we then did extra in class... to run another container, we pulled another image (=docker alpine) by following here (https://github.com/docker/labs/blob/master/beginner/chapters/alpine.md), in addition to scipy-notebook image.\n",
    "    5. in ec2 putty terminal, type 'docker pull alpine'\n",
    "    6. we basically followed the github website. after up to 'docker port static-site' code (to see where our static sites are at: 443/tcp->...32768, and 80/tcp->...32769, where the last 5digit numbers may be different each time), we go back gitbash (if its still within ec2 instance, which you can tell by [ec2-user@...] at the start of the cmd line, gotta exit the ec2 connection via 'exit'), and execute 'ssh -L 9999:localhost:8888 ec2-user@ec2-13-229-224-114.ap-southeast-1.compute.amazonaws.com -i kp1.pem', but 8888 gotta change to the '32769' seen a few lines above, all things btw 'ec2-user@' and '-i' is the Public DNS (IPv4) from your running AWS ec2 instance. The 'kp1.pem' after -i, is the name of your keypair as saved on your local computer. Now, in ec2 putty terminal, cmd line now starts with [ec2-user@...], means we're connected to ec2 instance! Now, if we type 'localhost:9999/' on a browser, we'll be on our static website! \n",
    "\n",
    "# 9.01 intro to bayes\n",
    "-https://www.khanacademy.org/math/ap-statistics/probability-ap/stats-conditional-probability/v/bayes-theorem-visualized , https://www.khanacademy.org/partner-content/wi-phi/wiphi-critical-thinking/wiphi-fundamentals/v/bayes-theorem , https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/\n",
    "\n",
    "-there are 2 approaches to stats: frequentist (depends on just the given sample or population), and bayes (depends on historical events, and dependance on other events)\n",
    "\n",
    "# 9.06 AB testing\n",
    "-https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/\n",
    "\n",
    "-factors: eg. subject headers, color of email image\n",
    "\n",
    "-levels: different types of subject headers, blue/red email image\n",
    "\n",
    "-epsilon greedy approach: a proportion (epsilon) of all versions of the email/product will be shown to users, the remaining versions take up a proportion of 1-epsilon of all emails/products. Epsilon changes as popularity/click-thru-rate of versions change, until eventually the best version has an epsilon of ~1, such that almost ALL emails/products sent are of the best version. This allows one to explore the effectiveness of different versions ('exploration'), while simultaneously allowing for usage of the best version ('exploitation'). \n",
    " \n",
    "# thoughts: what to do to remove highly correlated features, for different models\n",
    "\n",
    "-numerical (linreg/lasso/ridge): vif, rfe, rfecv\n",
    "\n",
    "-categorical (logreg/multinomialNB/bernoulliNB): selectkbest (chi2), chi2\n",
    "\n",
    "-nlp: chi2 (categorical)\n",
    "\n",
    "-decision tree (bagging, random forest, extra trees classifier/regressor): depends on whether its numerical/categorical? also, adaboost/gradientboost/xgboost doesnt really remove highly correlated features, but does improve score\n",
    "\n",
    "-svm (svc/svr): not really needed\n",
    "\n",
    "-gradient descent (batch/stochastic/mini batch): NA\n",
    "\n",
    "-generalized linear model\n",
    "\n",
    "# recap: regression or classifier?\n",
    "-regression: linreg, support vector REGRESSOR, decision tree/random forest/extra trees REGRESSOR, generalised linear model\n",
    "-classifier: logreg, support vector CLASSIFIER, decision tree/random forest/extra trees CLASSIFIER, generalised linear model, knn, naive bayes (multinomialNB/bernoulliNB), k-means, DBscan (more efficient than kmeans)\n",
    "-for both, behind the hood: gradient descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
